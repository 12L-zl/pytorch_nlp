{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d062dd",
   "metadata": {},
   "source": [
    "### 7.1 엘만 RNN의 문제점\n",
    "1. 멀리 떨어진 정보를 예측하지 못한다.\n",
    "* 정보의 유익성에 상관없이 은닉 상태 벡터 업데이트\n",
    "* 어떤 값을 유지하고 어떤 값을 버릴지 제어 X\n",
    "\n",
    "<br>\n",
    "\n",
    "2. 불안정한 그레이디언트\n",
    "* 렐루, 그레이디언트 클리핑, 게이팅 으로 해결\n",
    "\n",
    "### 7.2 엘만 RNN 문제 해결책 ; 게이팅\n",
    "* LSTM 신경망\n",
    "    * 조건에 따라 업데이트, 이전 은닉 상태의 값 의도적으로 지움\n",
    "    * 삭제 기능은 이전 은닉 상탯값과 또 다른 함수를 곱해 수행 된다.\n",
    "* GRU 신경망\n",
    "\n",
    "nn.RNN, nn.RNNCell -> nn.LSTM, nn.LSTMCell\n",
    "\n",
    "### 7.3 예제 : 문자 RNN으로 성씨 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774d6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741a3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_csv=\"./Data/surnames.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"./Data/surnames_with_splits.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = pd.read_csv(args.raw_dataset_csv, header=0)\n",
    "\n",
    "by_nationality = collections.defaultdict(list)\n",
    "for _, row in surnames.iterrows():\n",
    "    by_nationality[row.nationality].append(row.to_dict())\n",
    "    \n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_proportion*n)\n",
    "    n_val = int(args.val_proportion*n)\n",
    "    n_test = int(args.test_proportion*n)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "    \n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b62b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_surnames = pd.DataFrame(final_list)\n",
    "final_surnames.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_surnames.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406a2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object) :\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token  # 모델 파라미터 업데이트하는데 사용하지 않는 위치\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    \n",
    "    def add_token(self, token):        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "        \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76eb6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary) :\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token  # 임베딩층의 마스킹 역할, 가변 길이의 시퀀스의 손실 계산\n",
    "        self._unk_token = unk_token  # 드물게 등장하는 단어 학습\n",
    "        self._begin_seq_token = begin_seq_token  # 시퀀스 경계에 관한 힌트\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0710a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        # 성씨를 샘플과 타깃 벡터로 변환\n",
    "        # 성씨 벡터를 두 개의 벡터 surname[:-1], surname[1:]로 나누어 출력\n",
    "        # surname[:-1] : 샘플 / [1:] : 타깃\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) \n",
    "                       for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) - 1\n",
    "        \n",
    "        from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.char_vocab.mask_index\n",
    "\n",
    "        to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.char_vocab.mask_index\n",
    "        \n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(char_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "\n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "963f236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df \n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.validation_size), \n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 클래스 가중치\n",
    "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        from_vector, to_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
    "        \n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_data': from_vector, \n",
    "                'y_target': to_vector, \n",
    "                'class_index': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be859d",
   "metadata": {},
   "source": [
    "#### 1. 조건이 없는 모델\n",
    "* 성씨를 생성하기 전에 국적 정보 사용하지 X\n",
    "* GRU가 어떤 국적에도 편향된 계산 수행하지 X\n",
    "* 초기 은닉 상태 벡터가 계산에 영향을 미치지 않도록 모두 0으로 초기화\n",
    "* 초기 은닉 벡터가 모두 0이면 행렬 곱셈 결과도 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19f45197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerationModel(nn.Module) :\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\n",
    "                batch_first=True, padding_idx=0, dropout_p=0.5) :\n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # x_int : (batch, input_dim)\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded) # RNN 계열의 특성으로, 두개의 인자 반환 -> 출력 시퀀스, 최종은닉상태\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size) # 3차원으로 다시 변경\n",
    "            \n",
    "        return y_out # (batch, char_vocab_size(output_dim)) -> 3차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0f008999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 만든 인덱스 시퀀스 샘플링\n",
    "def sample_from_model(model, vectorizer, num_samples = 1, sample_size=20, temperature=1.0) :\n",
    "    '''\n",
    "    num_samples : 샘플 개수\n",
    "    sample_size : 샘플 최대 길이\n",
    "    temperature : 무작위성 정도\n",
    "    (0 ~ 1 사이면 최대값을 선택할 가능성이 높고, 1보다 크면 균등 분포에 가깝다)\n",
    "    '''\n",
    "    begin_seq_index = begin_seq_index = [vectorizer.char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)] # 시작 시퀀스 인덱스 반복하여 샘플 수만큼 생성\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    h_t = None # 초기 은닉 상태\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t) # 현재 인덱스를 임베딩 벡터로 변환\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "# 인덱스 -> 성씨 문자열 변환\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a3f4275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    if len(y_pred.size()) == 3: # 3차원 텐서이면, 행렬로 변환\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2: # 2차원 행렬이면, 벡터로 변환\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "# mask_index : 무시해야 할 특정 인덱스로, 패딩 토큰에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e936c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"./Data/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model5.pth\",\n",
    "    save_dir=\"./\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7e9198b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68882b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "50c6917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index = vectorizer.char_vocab.mask_index # 0\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "573254d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_in=batch_dict['x_data'])\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # 검증 세트에 대한 순회\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size = args.batch_size,\n",
    "                                           #device=args.device\n",
    "                                           )\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator) :\n",
    "            y_pred = model(x_in = batch_dict['x_data'])\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc)/ (batch_index + 1)\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "        # 샘플링을 위해 모델을 cpu로 옮깁니다\n",
    "        model = model.cpu()\n",
    "        sampled_surnames = decode_samples(\n",
    "            sample_from_model(model, vectorizer, num_samples=2), \n",
    "            vectorizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8086258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    # 이동 손실과 이동 정확도를 계산합니다\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "74ad1c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 2.5390436053276058;\n",
      "테스트 정확도: 25.193512687467052\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "53c87ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Allerryka\n",
      "Shiurron\n",
      "Seehivoki\n",
      "Jazholb\n",
      "Anchomaitd\n",
      "Vharge\n",
      "Moan\n",
      "Aingna\n",
      "Zovan\n",
      "Bares\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "\n",
    "# 생성할 이름 개수\n",
    "num_names = 10\n",
    "model = model.cpu()\n",
    "# 이름 생성\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_model(model, vectorizer, num_samples=num_names), \n",
    "    vectorizer)\n",
    "# 결과 출력\n",
    "print (\"-\"*15)\n",
    "for i in range(num_names):\n",
    "    print (sampled_surnames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2efee6",
   "metadata": {},
   "source": [
    "#### 2. 조건이 있는 모델\n",
    "* 성씨를 생성할 때 국적 고려\n",
    "* 모델이 특정 성씨에 상대적으로 편향될 수 있는 어떤 매커니즘이 있다는 의미\n",
    "* 모델 파라미터가 수정될 때 임베딩 행렬의 값도 조정\n",
    "* 국적 인덱스를 RNN 은닉 층과 같은 크기의 벡터로 매핑하는 Embedding 층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "12b0f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
    "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\n",
    "                                       embedding_dim=rnn_hidden_size) # RNN 은닉 상태 초기화하는 데 사용\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "        \n",
    "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\n",
    "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a89849c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, nationalities, sample_size=20, \n",
    "                      temperature=1.0):\n",
    "    num_samples = len(nationalities)\n",
    "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    h_t = model.nation_emb(nationality_indices)\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ca8978fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"./Data/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model6.pth\",\n",
    "    save_dir=\"./\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6e056f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),\n",
    "                               num_nationalities=len(vectorizer.nationality_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index,\n",
    "                               dropout_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8bf1f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "schedular = optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                                mode='min', factor=0.5,\n",
    "                                                patience=1)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059897d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs) :\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size,\n",
    "                                      #device=args.device\n",
    "                                      )\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator) :\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_in=batch_dict['x_data'], nationality_index=batch_dict['class_index'])\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                      )\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = model(x_in=batch_dict['x_data'],\n",
    "                      nationality_index=batch_dict['class_index'])\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=model, \n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "        \n",
    "    # 샘플링을 위해 모델을 cpu로 옮깁니다\n",
    "    nationalities = np.random.choice(np.arange(len(vectorizer.nationality_vocab)), replace=True, size=2)\n",
    "    model = model.cpu()\n",
    "    sampled_surnames = decode_samples(\n",
    "        sample_from_model(model, vectorizer, nationalities=nationalities), \n",
    "        vectorizer)\n",
    "\n",
    "    sample1 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[0]), \n",
    "                              sampled_surnames[0])\n",
    "    sample2 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[1]), \n",
    "                              sampled_surnames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = model(x_in=batch_dict['x_data'], \n",
    "                   nationality_index=batch_dict['class_index'])\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "    \n",
    "    # 이동 손실과 이동 정확도를 계산합니다\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링\n",
    "for index in range(len(vectorizer.nationality_vocab)):\n",
    "    nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    print(\"{} 샘플: \".format(nationality))\n",
    "    sampled_indices = sample_from_model(model, vectorizer,  \n",
    "                                        nationalities=[index] * 3, \n",
    "                                        temperature=0.7)\n",
    "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\n",
    "        print(\"-  \" + sampled_surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfff9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
