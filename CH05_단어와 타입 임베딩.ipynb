{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c403d0",
   "metadata": {},
   "source": [
    "### 5.1 임베딩 배우는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b16540ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b7001e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 단어 임베딩\n",
    "class PreTrainedEmbeddings(object) : # 임베딩 효율적으로 로드하고 처리\n",
    "    def __init__(self, word_to_index, word_vectors) :\n",
    "        '''\n",
    "        word_to_index : dict, 단어에서 정수로 매핑\n",
    "        word_vectors : numpy 배열의 리스트\n",
    "        '''\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v:k for k, v in self.word_to_index.items()}\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), 'euclidean') # vector_size\n",
    "        \n",
    "        for _, i in self.word_to_index.items() :\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        \n",
    "        self.index.build(50)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file) : #사전 훈련된 벡터파일에서 객체 생성\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "        with open(embedding_file, 'rt', encoding='UTF8') as fp :\n",
    "            for line in fp.readlines() :\n",
    "                line = line.split(' ')\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "                \n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "        \n",
    "        return cls(word_to_index, word_vectors)\n",
    "    \n",
    "    # 단어 임베딩 사용한 유추 작업\n",
    "    \n",
    "    def get_embedding(self, word) :\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "    \n",
    "    # 벡터 주어지면 n개의 최근접 이웃 반환\n",
    "    def get_closest_to_vector(self, vector, n=1) :\n",
    "        # vector는 annoy 인덱스에 있는 벡터의 크기와 같아야\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "    \n",
    "    # 단어 임베딩 사용한 유추 결과 출력\n",
    "    def compute_and_print_analogy(self, w1, w2, w3) :\n",
    "        # word1이 word2일 때, word3은 __word4__이다.\n",
    "        v1 = self.get_embedding(w1)\n",
    "        v2 = self.get_embedding(w2)\n",
    "        v3 = self.get_embedding(w3)\n",
    "        \n",
    "        spatial_relationship = v2 - v1\n",
    "        v4 = v3 + spatial_relationship\n",
    "        \n",
    "        closest_words = self.get_closest_to_vector(v4, n=4)\n",
    "        existing_words = set([w1, w2, w3])\n",
    "        closest_words = [word for word in closest_words\n",
    "                        if word not in existing_words]\n",
    "        \n",
    "        if len(closest_words) == 0 :\n",
    "            print('계산된 벡터와 가장 가까운 이웃 찾을 수 없다.')\n",
    "            return\n",
    "        \n",
    "        for w4 in closest_words :\n",
    "            print(\"{} : {} :: {} : {}\".format(w1, w2, w3, w4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "820efed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove 단어 임베딩 사용\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file('./Data/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "524b2634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : he :: woman : she\n",
      "man : he :: woman : her\n",
      "man : he :: woman : having\n",
      "--------------------\n",
      "fly : plane :: sail : ship\n",
      "fly : plane :: sail : vessel\n",
      "--------------------\n",
      "cat : kitten :: dog : puppy\n",
      "cat : kitten :: dog : rottweiler\n",
      "cat : kitten :: dog : puppies\n",
      "cat : kitten :: dog : pooch\n",
      "--------------------\n",
      "blue : color :: dog : animal\n",
      "blue : color :: dog : breed\n",
      "--------------------\n",
      "leg : legs :: hand : hands\n",
      "leg : legs :: hand : stick\n",
      "leg : legs :: hand : eyes\n",
      "--------------------\n",
      "toe : foot :: finger : turning\n",
      "toe : foot :: finger : moving\n",
      "toe : foot :: finger : attached\n",
      "--------------------\n",
      "talk : communicate :: read : interpret\n",
      "talk : communicate :: read : memorize\n",
      "talk : communicate :: read : typed\n",
      "--------------------\n",
      "blue : democrat :: red : republican\n",
      "blue : democrat :: red : congressman\n",
      "blue : democrat :: red : senator\n",
      "--------------------\n",
      "man : king :: woman : queen\n",
      "man : king :: woman : throne\n",
      "man : king :: woman : elizabeth\n",
      "--------------------\n",
      "man : doctor :: woman : nurse\n",
      "man : doctor :: woman : physician\n",
      "man : doctor :: woman : pregnant\n",
      "--------------------\n",
      "fast : fastest :: small : smallest\n",
      "fast : fastest :: small : largest\n",
      "fast : fastest :: small : quarters\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy('man', 'he', 'woman')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('cat', 'kitten', 'dog')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('blue', 'color', 'dog')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('leg', 'legs', 'hand')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('toe', 'foot', 'finger')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('talk', 'communicate', 'read')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('blue', 'democrat', 'red')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('man', 'king', 'woman')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('man', 'doctor', 'woman')\n",
    "print('-'*20)\n",
    "embeddings.compute_and_print_analogy('fast', 'fastest', 'small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9086d93",
   "metadata": {},
   "source": [
    "### 5.2 예제 : CBOW 임베딩 학습하기\n",
    "* 다중 분류 작업\n",
    "* 단어 텍스트 스캔해 단어의 문맥 윈도를 만든 후 문맥 윈도에서 **중앙의 단어 제거**하고 문맥 윈도 사용해 **누락된 단어 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c982d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑켄슈타인 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57b05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a532a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_txt=\"./Data/frankenstein.txt\",\n",
    "    window_size=5,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"./Data/frankenstein_with_splits.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeef1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3802eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3427 sentences\n",
      "Sample: No incidents have hitherto befallen us that would make a figure in a\n",
      "letter.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "with open(args.raw_dataset_txt) as fp:\n",
    "    book = fp.read()\n",
    "sentences = tokenizer.tokenize(book)\n",
    "\n",
    "print (len(sentences), \"sentences\")\n",
    "print (\"Sample:\", sentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ff860ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no incidents have hitherto befallen us that would make a figure in a letter . '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "cleaned_sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a365c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_27252\\2209348609.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sentence in tqdm_notebook(cleaned_sentences)])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9383b9ba6d4e40958301fd23f60778f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n만약, args.window_size = 2 이고, 문장이 hello world 이면,\\n<MASK> <MASK> hello world <MASK> <MASK>\\n윈도우 크기는 = 2 * 2 + 1 = 5\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 윈도 생성\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "\n",
    "flatten = lambda outer_list: [item for inner_list in outer_list for item in inner_list]\n",
    "windows = flatten([list(nltk.ngrams([MASK_TOKEN] * args.window_size + sentence.split(' ') + \\\n",
    "    [MASK_TOKEN] * args.window_size, args.window_size * 2 + 1)) \\\n",
    "    for sentence in tqdm_notebook(cleaned_sentences)])\n",
    "\n",
    "'''\n",
    "만약, args.window_size = 2 이고, 문장이 hello world 이면,\n",
    "<MASK> <MASK> hello world <MASK> <MASK>\n",
    "윈도우 크기는 = 2 * 2 + 1 = 5\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6537290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_27252\\4264982240.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for window in tqdm_notebook(windows):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c56081d1f5d4d878e4b4e5370880152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cbow data\n",
    "data = []\n",
    "for window in tqdm_notebook(windows):\n",
    "    target_token = window[args.window_size]\n",
    "    context = []\n",
    "    for i, token in enumerate(window):\n",
    "        if token == MASK_TOKEN or i == args.window_size:\n",
    "            continue\n",
    "        else:\n",
    "            context.append(token)\n",
    "    data.append([' '.join(token for token in context), target_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77f6414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the modern prometheus</td>\n",
       "      <td>frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern prometheus by</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus by mary</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by mary wo...</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frankenstein , or the prometheus by mary wolls...</td>\n",
       "      <td>modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>our email newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>email newsletter to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>newsletter to hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>to hear about new ebooks</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>hear about new ebooks .</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context        target\n",
       "0                             , or the modern prometheus  frankenstein\n",
       "1               frankenstein or the modern prometheus by             ,\n",
       "2           frankenstein , the modern prometheus by mary            or\n",
       "3      frankenstein , or modern prometheus by mary wo...           the\n",
       "4      frankenstein , or the prometheus by mary wolls...        modern\n",
       "...                                                  ...           ...\n",
       "90693         our email newsletter to hear new ebooks .          about\n",
       "90694           email newsletter to hear about ebooks .            new\n",
       "90695                    newsletter to hear about new .         ebooks\n",
       "90696                          to hear about new ebooks              .\n",
       "90697                            hear about new ebooks .              \n",
       "\n",
       "[90698 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_data = pd.DataFrame(data, columns=[\"context\", \"target\"])\n",
    "cbow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5367901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split data\n",
    "n = len(cbow_data)\n",
    "def get_split(row_num):\n",
    "    if row_num <= n*args.train_proportion:\n",
    "        return 'train'\n",
    "    elif (row_num > n*args.train_proportion) and (row_num <= n*args.train_proportion + n*args.val_proportion):\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'test'\n",
    "cbow_data['split']= cbow_data.apply(lambda row: get_split(row.name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "072f5d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the modern prometheus</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern prometheus by</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus by mary</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by mary wo...</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frankenstein , or the prometheus by mary wolls...</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>our email newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>email newsletter to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>newsletter to hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>to hear about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>hear about new ebooks .</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context        target  split\n",
       "0                             , or the modern prometheus  frankenstein  train\n",
       "1               frankenstein or the modern prometheus by             ,  train\n",
       "2           frankenstein , the modern prometheus by mary            or  train\n",
       "3      frankenstein , or modern prometheus by mary wo...           the  train\n",
       "4      frankenstein , or the prometheus by mary wolls...        modern  train\n",
       "...                                                  ...           ...    ...\n",
       "90693         our email newsletter to hear new ebooks .          about   test\n",
       "90694           email newsletter to hear about ebooks .            new   test\n",
       "90695                    newsletter to hear about new .         ebooks   test\n",
       "90696                          to hear about new ebooks              .   test\n",
       "90697                            hear about new ebooks .                 test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b713d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbow_data.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "852c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object) :\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token  # 모델 파라미터 업데이트하는데 사용하지 않는 위치\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    \n",
    "    def add_token(self, token):        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "        \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f240719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWVectorizer(object) :\n",
    "    def __init__(self, cbow_vocab):\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "    \n",
    "    # 문맥의 토큰 수가 최대 길이보다 작으면 0으로 채움 -> 패딩\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = \\\n",
    "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7dbe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split=='train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d7ef9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "class CBOWClassifier(nn.Module): \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        \n",
    "        # 문맥의 단어를 나타내는 인덱스를 각 단어의 벡터로 변환\n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "        # 출력 : (batch_size, context_size, embedding_size)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        # dim=1 : context_size(각 문맥의 단어들)에서 임베딩 벡터 합침\n",
    "        # 출력 (batch_size, embedding_size)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca4867db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17392c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    cbow_csv=\"./Data/frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model2.pth\",\n",
    "    save_dir=\"./\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    embedding_size=50,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c79db47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab), \n",
    "                            embedding_size=args.embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81f5cab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(6138, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=6138, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e25220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b3770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    \n",
    "    # 검증 세트에 대한 순회\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        y_pred =  classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=classifier,\n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74be466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc8560aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(x_in=batch_dict['x_data'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaaa7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 8.225977621639476;\n",
      "테스트 정확도: 12.124999999999996\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))\n",
    "\n",
    "# 형편없는 정확도 -> 모델 단순 + 적은 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f96a7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 결과 출력\n",
    "def pretty_print(results):\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f70ac76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[6.98] - ceased\n",
      "...[6.99] - situated\n",
      "...[6.99] - periodically\n",
      "...[7.05] - without\n",
      "...[7.07] - divine\n",
      "...[7.10] - seven\n",
      "=======monster=======\n",
      "...[6.51] - irresistible\n",
      "...[6.61] - dark\n",
      "...[6.65] - variety\n",
      "...[6.66] - spoken\n",
      "...[6.82] - wise\n",
      "...[6.92] - demonstrate\n",
      "=======science=======\n",
      "...[7.19] - should\n",
      "...[7.41] - colours\n",
      "...[7.58] - temptation\n",
      "...[7.59] - warring\n",
      "...[7.67] - hides\n",
      "...[7.72] - provided\n",
      "=======sickness=======\n",
      "...[6.43] - torrents\n",
      "...[6.55] - which\n",
      "...[6.68] - evidently\n",
      "...[6.74] - furtherance\n",
      "...[6.75] - rejoiced\n",
      "...[6.76] - shrieked\n",
      "=======lonely=======\n",
      "...[7.18] - omnipotent\n",
      "...[7.24] - expedition\n",
      "...[7.28] - gesture\n",
      "...[7.28] - hides\n",
      "...[7.29] - storm\n",
      "...[7.30] - decreasing\n",
      "=======happy=======\n",
      "...[6.88] - agitated\n",
      "...[6.91] - hero\n",
      "...[6.92] - comprehensive\n",
      "...[6.94] - event\n",
      "...[7.02] - conceited\n",
      "...[7.03] - creaking\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f16c2e",
   "metadata": {},
   "source": [
    "### 5.3 예제 :문서 분류에 사전 훈련된 임베딩을 사용한 전이 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8567f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_csv=\"./Data/news.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"./Data/news_with_splits.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55cd205a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                              title\n",
       "0       Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1       Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2       Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3       Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4       Business  Oil prices soar to all-time record, posing new...\n",
       "...          ...                                                ...\n",
       "119995     World  Pakistan's Musharraf Says Won't Quit as Army C...\n",
       "119996    Sports                  Renteria signing a top-shelf deal\n",
       "119997    Sports                    Saban not going to Dolphins yet\n",
       "119998    Sports                                  Today's NFL games\n",
       "119999    Sports                       Nets get Carter from Raptors\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(args.raw_dataset_csv, header=0)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46498174",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_category = collections.defaultdict(list)\n",
    "for _, row in news.iterrows():\n",
    "    by_category[row.category].append(row.to_dict()) # category : title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5904e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business: 30000\n",
      "Sci/Tech: 30000\n",
      "Sports: 30000\n",
      "World: 30000\n",
      "0: 0\n",
      "1: 0\n",
      "100: 0\n"
     ]
    }
   ],
   "source": [
    "for category in by_category:\n",
    "        print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "869e7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split data\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "for _, item_list in by_category.items():\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_proportion*n)\n",
    "    n_val = int(args.val_proportion*n)\n",
    "    n_test = int(args.test_proportion*n)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train] :\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val] :\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:] :\n",
    "        item['split'] = 'test'  \n",
    "    \n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cec6fd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Jobs, tax cuts key issues for Bush</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Jarden Buying Mr. Coffee #39;s Maker</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Retail sales show festive fervour</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Intervoice's Customers Come Calling</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Boeing Expects Air Force Contract</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>World</td>\n",
       "      <td>Genesis Space Capsule Crashes Into Desert</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>World</td>\n",
       "      <td>U.S.: Too Early to Tell Iraq Unit's Fate</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>World</td>\n",
       "      <td>AFGHAN OPIUM GROWING UP TWO THIRDS</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>World</td>\n",
       "      <td>At least one Saudi policeman killed in clashes...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>World</td>\n",
       "      <td>U.S. Forces Claim Most of Fallujah</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                              title  split\n",
       "0       Business                 Jobs, tax cuts key issues for Bush  train\n",
       "1       Business               Jarden Buying Mr. Coffee #39;s Maker  train\n",
       "2       Business                  Retail sales show festive fervour  train\n",
       "3       Business                Intervoice's Customers Come Calling  train\n",
       "4       Business                  Boeing Expects Air Force Contract  train\n",
       "...          ...                                                ...    ...\n",
       "119995     World          Genesis Space Capsule Crashes Into Desert   test\n",
       "119996     World           U.S.: Too Early to Tell Iraq Unit's Fate   test\n",
       "119997     World                 AFGHAN OPIUM GROWING UP TWO THIRDS   test\n",
       "119998     World  At least one Saudi policeman killed in clashes...   test\n",
       "119999     World                 U.S. Forces Claim Most of Fallujah   test\n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_news = pd.DataFrame(final_list)\n",
    "final_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb9c3a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    84000\n",
       "val      18000\n",
       "test     18000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_news.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08b68325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>jobs , tax cuts key issues for bush</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>jarden buying mr . coffee s maker</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>retail sales show festive fervour</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>intervoice s customers come calling</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>boeing expects air force contract</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>World</td>\n",
       "      <td>genesis space capsule crashes into desert</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>World</td>\n",
       "      <td>u . s . too early to tell iraq unit s fate</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>World</td>\n",
       "      <td>afghan opium growing up two thirds</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>World</td>\n",
       "      <td>at least one saudi policeman killed in clashes...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>World</td>\n",
       "      <td>u . s . forces claim most of fallujah</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                              title  split\n",
       "0       Business                jobs , tax cuts key issues for bush  train\n",
       "1       Business                  jarden buying mr . coffee s maker  train\n",
       "2       Business                  retail sales show festive fervour  train\n",
       "3       Business                intervoice s customers come calling  train\n",
       "4       Business                  boeing expects air force contract  train\n",
       "...          ...                                                ...    ...\n",
       "119995     World          genesis space capsule crashes into desert   test\n",
       "119996     World         u . s . too early to tell iraq unit s fate   test\n",
       "119997     World                 afghan opium growing up two thirds   test\n",
       "119998     World  at least one saudi policeman killed in clashes...   test\n",
       "119999     World              u . s . forces claim most of fallujah   test\n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "final_news.title = final_news.title.apply(preprocess_text)\n",
    "\n",
    "final_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7657726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_news.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a2d174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary 클래스 상속\n",
    "# 단어를 정수 시퀀스에 매핑\n",
    "class SequenceVocabulary(Vocabulary) :\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token  # 임베딩층의 마스킹 역할, 가변 길이의 시퀀스의 손실 계산\n",
    "        self._unk_token = unk_token  # 드물게 등장하는 단어 학습\n",
    "        self._begin_seq_token = begin_seq_token  # 시퀀스 경계에 관한 힌트\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f67394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "class NewsVectorizer(object) :\n",
    "    def __init__(self, title_vocab, category_vocab) :\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "        \n",
    "    def vectorize(self, title, vector_length = -1) :\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        indices.extend(self.title_vocab.lookup_token(token)\n",
    "                      for token in title.split(' '))\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "        \n",
    "        if vector_length < 0 :\n",
    "            vector_length = len(indices)\n",
    "            \n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices) :] = self.title_vocab.mask_index\n",
    "        \n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25) :\n",
    "        category_vocab = Vocabulary()\n",
    "        for category in sorted(set(news_df.category)) :\n",
    "            category_vocab.add_token(category)\n",
    "            \n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title :\n",
    "            for token in title.split(' ') :\n",
    "                if token not in string.punctuation :\n",
    "                    word_counts[token] += 1\n",
    "                    \n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items() :\n",
    "            if word_count >= cutoff :\n",
    "                title_vocab.add_token(word)\n",
    "                \n",
    "        return cls(title_vocab, category_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents) :\n",
    "        title_vocab=SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        \n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self) :\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bb81d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class NewsDataset(Dataset) :\n",
    "    def __init__(self, news_df, vectorizer) :\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "        # begin_seq만 사용하면 +1, end_seq까지 같이 사용하면 +2\n",
    "        \n",
    "        self.train_df = self.news_df[self.news_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 클래스 가중치\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_news_df = news_df[news_df.split=='train']\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        title_vector = \\\n",
    "            self._vectorizer.vectorize(row.title, self._max_seq_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': title_vector,\n",
    "                'y_target': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89d307c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0) :\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None :\n",
    "            self.emb = nn.Embedding(embedding_dim = embedding_size,\n",
    "                                   num_embeddings = num_embeddings,\n",
    "                                   padding_idx = padding_idx)\n",
    "            \n",
    "        else :\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim = embedding_size,\n",
    "                                   num_embeddings = num_embeddings,\n",
    "                                   padding_idx = padding_idx,\n",
    "                                   _weight = pretrained_embeddings)\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size, \n",
    "                   out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False) :\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1) # 특성과 채널 차원 변경\n",
    "        # (batch_size, sequence_length, embedding_size) -> (batch_size, emb_size, sequence_length)\n",
    "        # Conv1D레이어는 채널이 두 번째 차원에 있어야 함\n",
    "        features = self.convnet(x_embedded)\n",
    "        \n",
    "        # 평균 값 계산해 부가적인 차원 제거\n",
    "        remaining_size = features.size(dim=2) # features의 세번째 차원 sequence_length 크기 반환\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2) # 평균 풀링의 커널 크기 = remaining_size\n",
    "        # 평균 풀링 후 출력 (batch_size, num_channels, 1) -> squeeze(dim=2)\n",
    "        # -> (batch_size, num_channels)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # MLP 분류기\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3649cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전에 기반하여 단어 임베딩 부분 집합 선택\n",
    "def load_glove_from_file(glove_filepath) :\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, 'rt', encoding='UTF8') as fp :\n",
    "        for index, line in enumerate(fp) :\n",
    "            line = line.split(' ')\n",
    "            word_to_index[line[0]] = index # word = line[0] # 첫번째 요소는 단어\n",
    "            embedding_i = np.array([float(val) for val in line[1:]]) # 나머지 요소는 임베딩 벡터\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "# 특정 단어 집합에 대한 임베딩 행렬 생성\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            # 데이터셋에는 있지만 사전 훈련된 GloVe 임베딩에 없는 단어 등장하면 문제 발생\n",
    "            # Xavier 균등 분포로 초기화\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "284a0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    news_csv=\"./Data/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model3.pth\",\n",
    "    save_dir=\"./\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    glove_filepath='./Data/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # 실행 옵션\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c135082",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_glove = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "526bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "afa97f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 훈련된 임베딩을 사용합니다\n"
     ]
    }
   ],
   "source": [
    "# GloVe를 사용하거나 랜덤하게 임베딩을 초기화합니다\n",
    "if args.use_glove:\n",
    "    words = vectorizer.title_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"사전 훈련된 임베딩을 사용합니다\")\n",
    "else:\n",
    "    print(\"사전 훈련된 임베딩을 사용하지 않습니다\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = NewsClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.title_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.category_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a9f47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cda0ea81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all 6 classes or no classes but got weight tensor of shape: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier(batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_data\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 단계 3. 손실을 계산합니다\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_target\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m loss_t \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss_t \u001b[38;5;241m-\u001b[39m running_loss) \u001b[38;5;241m/\u001b[39m (batch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: weight tensor should be defined either for all 6 classes or no classes but got weight tensor of shape: [4]"
     ]
    }
   ],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be5278",
   "metadata": {},
   "source": [
    "* cuda 설치 못함 이슈로 실행 불가 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "#classifier = classifier.to(args.device)\n",
    "#dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b41083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))\n",
    "\n",
    "#테스트 손실: 0.6171224400401114\n",
    "#테스트 정확도: 79.64843749999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e1e0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 뉴스 제목의 카테고리 예측하기\n",
    "def predict_category(title, classifier, vectorizer, max_length) :\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33c0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.title[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f71090",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length + 1)\n",
    "        print(\"예측: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + 샘플: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
