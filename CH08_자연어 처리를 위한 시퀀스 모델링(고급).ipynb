{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983b8c9d",
   "metadata": {},
   "source": [
    "### 8.1 시퀀스-투-시퀀스 모델, 인코더-디코더 모델, 조건부 생성\n",
    "* 인코더 : 입력에서 현재 문제와 관련된 중요한 성질 감지\n",
    "* 디코더 : 인코딩된 입력을 받아 원하는 출력 만듦\n",
    "* 입력과 출력 사이의 매핑 : 정렬\n",
    "\n",
    "[S2S 모델의 핵심 구성 요소]\n",
    "* 양방향 순환 모델\n",
    "* 어텐션\n",
    "\n",
    "### 8.2 강력한 시퀀스 모델링 : 양방향 순환 모델\n",
    "* 과거와 미래의 정보를 합침\n",
    "\n",
    "### 8.3 강력한 시퀀스 모델링 : 어텐션\n",
    "* 인코딩에 최종 은닉 상태만 사용하는 제약이 있기에, 긴 문장에서 전체 입력 정보 감지 어려움\n",
    "* 출력을 생성할 때 관련된 입력 부분에 초점을 맞추는 어텐션\n",
    "\n",
    "\n",
    "1. 심층 신경망의 어텐션\n",
    "* 각 타임 스텝은 은닉 상태 생성\n",
    "* 어텐션을 사용할 때는 인코더의 최종 은닉 상태뿐만 아니라 중간 타임 스텝의 은닉 상태도 고려\n",
    "\n",
    "\n",
    "[용어정리]\n",
    "* 인코더의 은닉 상태 = 값 = 키\n",
    "* 디코더의 이전 은닉 상태 = 쿼리\n",
    "* 어텐션은 주의를 기울이려는 값의 개수와 차원이 같은 벡터 하나로 표현되는데, 이를 어텐션 벡터 = 어텐션 가중치 = 정렬 이라 한다.\n",
    "* 어텐션 가중치 + 인코더 상태(값) = 문맥 벡터 = 글림스 -> 디코더의 입력\n",
    "* 다음 타임 스텝의 어텐션 벡터는 호환성 함수를 사용해 업데이트\n",
    "\n",
    "--------------\n",
    "* 콘텐츠 인식 어텐션  \n",
    "* 위치 인식 어텐션 : 쿼리 벡터 + 키만 사용  \n",
    "* 소프트 어텐션 : 어텐션 가중치는 일반적으로 0 ~ 1 사이 실수  \n",
    "* 하드 어텐션 : 0 or 1인 이진 벡터 학습  \n",
    "* 전역 어텐션 : 입력의 모든 타임 스텝에 대해 인코더의 상태 사용  \n",
    "* 지역 어텐션 : 현재 타임 스텝 주위에 있는 입력에만 의존  \n",
    "* 지도 어텐션 : 동시에 훈련도는 별도의 신경망 사용해 어텐션 함수 학습  \n",
    "* 멀티헤드 어텐션 : 트랜스포머 네트워크, 여러 어텐션 벡터 사용해 입력의 다양한 영역 추적  \n",
    "* 셀프 어텐션 : 입력의 어떤 영역이 다른 영역에 영향을 미치는지 학습  \n",
    "* 멀티모달 어텐션 : 이미지와 음성처럼 입력의 형태 다양할 때  \n",
    "\n",
    "\n",
    "### 8.4 시퀀스 생성 모델 평가\n",
    "* 참조 출력이라는 기대 출력으로 평가\n",
    "* 모델의 출력이 얼마나 참조 출력에 가까운지 점수로 매김\n",
    "1. 사람평가  \n",
    "*  좋음 / 나쁨 을 표시하거나 번역 고치는 방법\n",
    "* 에러율, 평가자 간의 일치율\n",
    "* HTER\n",
    "2. 자동 평가\n",
    "* n-그램 중복 기반 지표\n",
    "    * 참조와 출력이 얼마나 가까운지 n-그램 중복 통계로 점수 계산\n",
    "    * BLEU, ROUGE, METEOR\n",
    "* 혼란도\n",
    "    * 출력 시퀀스의 확률을 측정할 수 있다면 적용 가능\n",
    "    * 과장된 지표, 모델의 오차율에 직접 반영되지 않음\n",
    "    \n",
    "### 8.5 예제 : 신경망 기계 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23a1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d92b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c115cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01c694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4])\n",
      "Values: \n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 0.],\n",
      "        [8., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "abcd_padded = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "efg_padded = torch.tensor([5, 6, 7, 0], dtype=torch.float32)\n",
    "h_padded = torch.tensor([8, 0, 0, 0], dtype=torch.float32)\n",
    "\n",
    "padded_tensor = torch.stack([abcd_padded, efg_padded, h_padded])\n",
    "\n",
    "describe(padded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8968f9",
   "metadata": {},
   "source": [
    "* pack_padded_sequence : 패딩된 시퀀스 압축\n",
    "1. 0으로 패딩이 많이 된 것은 밑으로  \n",
    "1 2 3 4  - 4  \n",
    "5 6 7 0  - 3  \n",
    "8 0 0 0  - 1  \n",
    "2. batch_first = True -> 첫 번째 차원이 배치 차원인지\n",
    "* 세로로 읽기\n",
    "1 5 8 2 6 3 7 4\n",
    "3. batch_size는 세로로 읽기  \n",
    "**3**(158) **2**(26) **2**(37) **1**(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b886b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1., 5., 8., 2., 6., 3., 7., 4.]), batch_sizes=tensor([3, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [4, 3, 1]\n",
    "packed_tensor = pack_padded_sequence(padded_tensor, lengths, \n",
    "                                     batch_first=True)\n",
    "packed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d940f",
   "metadata": {},
   "source": [
    "* pad_packed_sequence : 압축된 시퀀스 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35bba359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4])\n",
      "Values: \n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 0.],\n",
      "        [8., 0., 0., 0.]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([3])\n",
      "Values: \n",
      "tensor([4, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "unpacked_tensor, unpacked_lengths = pad_packed_sequence(packed_tensor, batch_first=True)\n",
    "describe(unpacked_tensor)\n",
    "describe(unpacked_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a44ca9",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a0105d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30a0c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    source_data_path=\"./Data/eng-fra.txt\",\n",
    "    output_data_path=\"./Data/simplest_eng_fra.csv\",\n",
    "    perc_train=0.7,\n",
    "    perc_val=0.15,\n",
    "    perc_test=0.15,\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "assert args.perc_test > 0 and (args.perc_test + args.perc_val + args.perc_train == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2daa8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.source_data_path, 'r', encoding='UTF8') as fp:\n",
    "    lines = fp.readlines()\n",
    "    \n",
    "lines = [line.replace(\"\\n\", \"\").lower().split(\"\\t\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88de7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for english_sentence, french_sentence in lines:\n",
    "    data.append({\"english_tokens\": word_tokenize(english_sentence, language=\"english\"),\n",
    "                 \"french_tokens\": word_tokenize(french_sentence, language=\"french\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ec9277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'english_tokens': ['go', '.'], 'french_tokens': ['va', '!']},\n",
       " {'english_tokens': ['run', '!'], 'french_tokens': ['cours', '!']},\n",
       " {'english_tokens': ['run', '!'], 'french_tokens': ['courez', '!']},\n",
       " {'english_tokens': ['wow', '!'], 'french_tokens': ['ça', 'alors', '!']},\n",
       " {'english_tokens': ['fire', '!'], 'french_tokens': ['au', 'feu', '!']}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a881e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_phrases = (\n",
    "    (\"i\", \"am\"), (\"i\", \"'m\"), \n",
    "    (\"he\", \"is\"), (\"he\", \"'s\"),\n",
    "    (\"she\", \"is\"), (\"she\", \"'s\"),\n",
    "    (\"you\", \"are\"), (\"you\", \"'re\"),\n",
    "    (\"we\", \"are\"), (\"we\", \"'re\"),\n",
    "    (\"they\", \"are\"), (\"they\", \"'re\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bd389b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = {phrase: [] for phrase in filter_phrases}\n",
    "for datum in data:\n",
    "    key = tuple(datum['english_tokens'][:2]) # ('go', '.')\n",
    "    if key in data_subset:\n",
    "        data_subset[key].append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0f1b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('i', 'am'): 805,\n",
       "  ('i', \"'m\"): 4760,\n",
       "  ('he', 'is'): 1069,\n",
       "  ('he', \"'s\"): 787,\n",
       "  ('she', 'is'): 504,\n",
       "  ('she', \"'s\"): 316,\n",
       "  ('you', 'are'): 449,\n",
       "  ('you', \"'re\"): 2474,\n",
       "  ('we', 'are'): 181,\n",
       "  ('we', \"'re\"): 1053,\n",
       "  ('they', 'are'): 194,\n",
       "  ('they', \"'re\"): 470},\n",
       " 13062)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {k: len(v) for k,v in data_subset.items()}\n",
    "counts, sum(counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92f028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "\n",
    "dataset_stage3 = []\n",
    "for phrase, datum_list in sorted(data_subset.items()):\n",
    "    np.random.shuffle(datum_list)\n",
    "    n_train = int(len(datum_list) * args.perc_train)\n",
    "    n_val = int(len(datum_list) * args.perc_val)\n",
    "\n",
    "    for datum in datum_list[:n_train]:\n",
    "        datum['split'] = 'train'\n",
    "        \n",
    "    for datum in datum_list[n_train:n_train+n_val]:\n",
    "        datum['split'] = 'val'\n",
    "        \n",
    "    for datum in datum_list[n_train+n_val:]:\n",
    "        datum['split'] = 'test'\n",
    "    \n",
    "    dataset_stage3.extend(datum_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa1c22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in dataset_stage3:\n",
    "    datum['source_language'] = \" \".join(datum.pop('english_tokens'))\n",
    "    datum['target_language'] = \" \".join(datum.pop('french_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e09f46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'split': 'train',\n",
       "  'source_language': \"he 's the cutest boy in town .\",\n",
       "  'target_language': \"c'est le garçon le plus mignon en ville .\"},\n",
       " {'split': 'train',\n",
       "  'source_language': \"he 's a nonsmoker .\",\n",
       "  'target_language': 'il est non-fumeur .'},\n",
       " {'split': 'train',\n",
       "  'source_language': \"he 's smarter than me .\",\n",
       "  'target_language': 'il est plus intelligent que moi .'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_stage3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f2a6864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's the cutest boy in town .</td>\n",
       "      <td>c'est le garçon le plus mignon en ville .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a nonsmoker .</td>\n",
       "      <td>il est non-fumeur .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's smarter than me .</td>\n",
       "      <td>il est plus intelligent que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a lovely young man .</td>\n",
       "      <td>c'est un adorable jeune homme .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's three years older than me .</td>\n",
       "      <td>il a trois ans de plus que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13057</th>\n",
       "      <td>test</td>\n",
       "      <td>you are n't invited .</td>\n",
       "      <td>vous n'êtes pas invités .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058</th>\n",
       "      <td>test</td>\n",
       "      <td>you are always watching tv .</td>\n",
       "      <td>tu regardes tout le temps la télé .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13059</th>\n",
       "      <td>test</td>\n",
       "      <td>you are trusted by every one of us .</td>\n",
       "      <td>chacun de nous te fait confiance .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13060</th>\n",
       "      <td>test</td>\n",
       "      <td>you are blinded by love .</td>\n",
       "      <td>vous êtes aveuglé par l'amour .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13061</th>\n",
       "      <td>test</td>\n",
       "      <td>you are a good person .</td>\n",
       "      <td>tu es une chouette gonzesse .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13062 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       split                       source_language  \\\n",
       "0      train        he 's the cutest boy in town .   \n",
       "1      train                   he 's a nonsmoker .   \n",
       "2      train               he 's smarter than me .   \n",
       "3      train            he 's a lovely young man .   \n",
       "4      train     he 's three years older than me .   \n",
       "...      ...                                   ...   \n",
       "13057   test                 you are n't invited .   \n",
       "13058   test          you are always watching tv .   \n",
       "13059   test  you are trusted by every one of us .   \n",
       "13060   test             you are blinded by love .   \n",
       "13061   test               you are a good person .   \n",
       "\n",
       "                                 target_language  \n",
       "0      c'est le garçon le plus mignon en ville .  \n",
       "1                            il est non-fumeur .  \n",
       "2              il est plus intelligent que moi .  \n",
       "3                c'est un adorable jeune homme .  \n",
       "4               il a trois ans de plus que moi .  \n",
       "...                                          ...  \n",
       "13057                  vous n'êtes pas invités .  \n",
       "13058        tu regardes tout le temps la télé .  \n",
       "13059         chacun de nous te fait confiance .  \n",
       "13060            vous êtes aveuglé par l'amour .  \n",
       "13061              tu es une chouette gonzesse .  \n",
       "\n",
       "[13062 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_df = pd.DataFrame(dataset_stage3)\n",
    "nmt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e609ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmt_df.to_csv(args.output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36106fe1",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e32e415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8667691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object) :\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token  # 모델 파라미터 업데이트하는데 사용하지 않는 위치\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    \n",
    "    def add_token(self, token):        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9f29672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary) :\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token  # 임베딩층의 마스킹 역할, 가변 길이의 시퀀스의 손실 계산\n",
    "        self._unk_token = unk_token  # 드물게 등장하는 단어 학습\n",
    "        self._begin_seq_token = begin_seq_token  # 시퀀스 경계에 관한 힌트\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983459e",
   "metadata": {},
   "source": [
    "[소스 영어와 타깃 프랑스어를 벡터로 벼환하려면 복잡도가 증가한다.]\n",
    "* 소스와 타깃 시퀀스는 모델에서 다른 역할을 하고, 언어가 다르며, 다른 두 방식으로 벡터화된다.\n",
    "* 소스 시퀀스의 길이에 따라 각 미니배치 소팅해야 한다.  \n",
    "=> 두 문제에 대비하려고 객체 두 개를 만들고, 최대 시퀀스 길이를 따로 측정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58f04955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "class NMTVectorizer(object) :\n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length, max_target_length) :\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        \n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    # 인덱스 -> 벡터\n",
    "    # 고정크기 벡터로 변환하고, 필요시 mask_index로 패딩\n",
    "    # S2S 모델의 디코더가 예측 작업에 타임 스텝마다 입력/출력 토큰 필요함의 작업을 수행하지만, 인코더 문맥이 추가된다.\n",
    "    # 이런 작업 단순화하고자, 소스와 타깃 인덱스에 상관없이 벡터화를 수행\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    # 벡터로 변환된 소스 텍스트 반환\n",
    "    def _get_source_indices(self, text):\n",
    "        indices = [self.source_vocab.begin_seq_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in text.split(\" \"))\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        return indices\n",
    "        \n",
    "    # 벡터로 변환된 타깃 텍스트 반환\n",
    "    def _get_target_indices(self, text):\n",
    "        indices = [self.target_vocab.lookup_token(token) for token in text.split(\" \")]\n",
    "        x_indices = [self.target_vocab.begin_seq_index] + indices\n",
    "        y_indices = indices + [self.target_vocab.end_seq_index]\n",
    "        return x_indices, y_indices # 디코더에서 샘플/예측을 나타내는 정수 리스트\n",
    "    \n",
    "    # 벡터화된 소스 텍스트와 타깃 텍스트 반환\n",
    "    # 벡터화된 소스 텍스트는 하나의 벡터, 타깃 텍스트는 2개의 벡터(샘플, 타깃)\n",
    "    def vectorize(self, source_text, target_text, use_dataset_max_lengths=True): # 최대 벡터 길이 사용할지 여부\n",
    "        source_vector_length = -1\n",
    "        target_vector_length = -1\n",
    "        \n",
    "        if use_dataset_max_lengths:\n",
    "            source_vector_length = self.max_source_length + 2 # begin / end index\n",
    "            target_vector_length = self.max_target_length + 1 # 토큰 하나가 밀린 복사본 두 개로 벡터화(첫번째 복사본-begin_of_seq, 두번째 복사본-end)\n",
    "            \n",
    "        source_indices = self._get_source_indices(source_text)\n",
    "        source_vector = self._vectorize(source_indices, \n",
    "                                        vector_length=source_vector_length, \n",
    "                                        mask_index=self.source_vocab.mask_index)\n",
    "        \n",
    "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
    "        target_x_vector = self._vectorize(target_x_indices,\n",
    "                                        vector_length=target_vector_length,\n",
    "                                        mask_index=self.target_vocab.mask_index)\n",
    "        target_y_vector = self._vectorize(target_y_indices,\n",
    "                                        vector_length=target_vector_length,\n",
    "                                        mask_index=self.target_vocab.mask_index)\n",
    "        return {\"source_vector\": source_vector, \n",
    "                \"target_x_vector\": target_x_vector, \n",
    "                \"target_y_vector\": target_y_vector, \n",
    "                \"source_length\": len(source_indices)}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        \n",
    "        max_source_length = 0\n",
    "        max_target_length = 0\n",
    "\n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = row[\"source_language\"].split(\" \")\n",
    "            if len(source_tokens) > max_source_length:\n",
    "                max_source_length = len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            \n",
    "            target_tokens = row[\"target_language\"].split(\" \")\n",
    "            if len(target_tokens) > max_target_length:\n",
    "                max_target_length = len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "            \n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        \n",
    "        return cls(source_vocab=source_vocab, \n",
    "                   target_vocab=target_vocab, \n",
    "                   max_source_length=contents[\"max_source_length\"], \n",
    "                   max_target_length=contents[\"max_target_length\"])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"source_vocab\": self.source_vocab.to_serializable(), \n",
    "                \"target_vocab\": self.target_vocab.to_serializable(), \n",
    "                \"max_source_length\": self.max_source_length,\n",
    "                \"max_target_length\": self.max_target_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b75829e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        self.text_df = text_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.text_df[self.text_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.text_df[self.text_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.text_df[self.text_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        train_subset = text_df[text_df.split=='train']\n",
    "        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(text_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NMTVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        vector_dict = self._vectorizer.vectorize(row.source_language, row.target_language)\n",
    "\n",
    "        return {\"x_source\": vector_dict[\"source_vector\"], \n",
    "                \"x_target\": vector_dict[\"target_x_vector\"],\n",
    "                \"y_target\": vector_dict[\"target_y_vector\"], \n",
    "                \"x_source_length\": vector_dict[\"source_length\"]}\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35dc93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nmt_batches(dataset, batch_size, shuffle=True,\n",
    "                        drop_last = True, device='cpu') :\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b57703",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94277d6",
   "metadata": {},
   "source": [
    "1. NMTEncoder : 소스 시퀀스를 입력으로 받아 임베딩하여 양방향 GRU에 주입\n",
    "2. NMTDecoder\n",
    "* 인코더 상태와 어텐션을 사용해 디코더가 새로운 시퀀스 생성\n",
    "* 타임 스텝마다 정답 타깃 시퀀스를 입력으로 사용\n",
    "* 또는 디코더가 선택한 시퀀스를 입력으로 사용\n",
    "* 이를 커리큘럼 학습, 탐색 학습이라 함\n",
    "3. NMTModel : 인코더와 디코더를 하나의 클래스로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf6ea290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(nn.Module) :\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        \n",
    "        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
    "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "    \n",
    "    def forward(self, x_source, x_lengths):\n",
    "        x_embedded = self.source_embedding(x_source)\n",
    "        \n",
    "        # PackedSequence 생성; x_packed.data.shape=(number_items, embedding_size)\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(), \n",
    "                                        batch_first=True)\n",
    "        \n",
    "        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n",
    "        x_birnn_out, x_birnn_h  = self.birnn(x_packed)\n",
    "        \n",
    "        # (batch_size, num_rnn, feature_size)로 변환\n",
    "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
    "        \n",
    "        # 특성 펼침; (batch_size, num_rnn * feature_size)로 바꾸기\n",
    "        # (참고: -1은 남은 차원에 해당합니다, \n",
    "        #       두 개의 RNN 은닉 벡터를 1로 펼칩니다)\n",
    "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
    "        \n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
    "        \n",
    "        return x_unpacked, x_birnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "097f7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원소별 연산을 사용하는 어텐션 매커니즘 버전\n",
    "# 텐서가 다른 텐서에 브로드캐스팅될 수 있도록 view() 연산을 사용해 크기가 1인 차원 추가\n",
    "def verbose_attention(encoder_state_vectors, query_vector):\n",
    "    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n",
    "    vector_scores = torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), \n",
    "                              dim=2)  # (batch_size, num_vectors)\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
    "    weighted_vectors = encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n",
    "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
    "    return context_vectors, vector_probabilities, vector_scores\n",
    "\n",
    "\n",
    "# 점곱을 사용하는 어텐션 매커니즘 버전\n",
    "# view()연산을 unsqueeze()로 변환\n",
    "# 원소별 곱셈과 덧셈 대신 matmul()연산 사용\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=-1) # (batch_size, num_vectors)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1), \n",
    "                                   vector_probabilities.unsqueeze(dim=2)).squeeze() # (batch_size, vector_size)\n",
    "    return context_vectors, vector_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61d8323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                             embedding_dim=embedding_size, \n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, \n",
    "                                   rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index # begin of sequence 인덱스\n",
    "    \n",
    "    # Begin_of_sequence 인덱스 벡터 반환\n",
    "    def _init_indices(self, batch_size) :\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "    \n",
    "    # 문맥 벡터 초기화하기 위한 0벡터 반환\n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
    "        # 가정: 첫 번째 차원은 배치 차원입니다\n",
    "        # 즉 입력은 (Batch, Seq)\n",
    "        # 시퀀스에 대해 반복해야 하므로 (Seq, Batch)로 차원을 바꿉니다\n",
    "        target_sequence = target_sequence.permute(1, 0)\n",
    "        output_sequence_size = target_sequence.size(0)\n",
    "\n",
    "        # 주어진 인코더의 은닉 상태를 초기 은닉 상태로 사용합니다\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "\n",
    "        batch_size = encoder_state.size(0)\n",
    "        # 문맥 벡터를 0으로 초기화합니다\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        # 첫 단어 y_t를 BOS로 초기화합니다\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            y_t_index = target_sequence[i]\n",
    "                \n",
    "            # 단계 1: 단어를 임베딩하고 이전 문맥과 연결합니다\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # 단계 2: GRU를 적용하고 새로운 은닉 벡터를 얻습니다\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            \n",
    "            # 단계 3: 현재 은닉 상태를 사용해 인코더의 상태를 주목합니다\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state, \n",
    "                                                           query_vector=h_t)\n",
    "            \n",
    "            # 부가 작업: 시각화를 위해 어텐션 확률을 저장합니다\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # 단게 4: 현재 은닉 상태와 문맥 벡터를 사용해 다음 단어를 예측합니다\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            \n",
    "            # 부가 작업: 예측 성능 점수를 기록합니다\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        \n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29ef0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module) :\n",
    "    def __init__(self, source_vocab_size, source_embedding_size, \n",
    "                 target_vocab_size, target_embedding_size, encoding_size, \n",
    "                 target_bos_index):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = NMTEncoder(num_embeddings=source_vocab_size, \n",
    "                                  embedding_size=source_embedding_size,\n",
    "                                  rnn_hidden_size=encoding_size)\n",
    "        decoding_size = encoding_size * 2\n",
    "        self.decoder = NMTDecoder(num_embeddings=target_vocab_size, \n",
    "                                  embedding_size=target_embedding_size, \n",
    "                                  rnn_hidden_size=decoding_size,\n",
    "                                  bos_index=target_bos_index)\n",
    "    \n",
    "    \n",
    "    def forward(self, x_source, x_source_lengths, target_sequence):\n",
    "        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\n",
    "        decoded_states = self.decoder(encoder_state=encoder_state, \n",
    "                                      initial_hidden_state=final_hidden_states, \n",
    "                                      target_sequence=target_sequence)\n",
    "        return decoded_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "afd34192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a858193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(dataset_csv=\"./Data/simplest_eng_fra.csv\",\n",
    "                 vectorizer_file=\"vectorizer.json\",\n",
    "                 model_state_file=\"model7.pth\",\n",
    "                 save_dir=\"./\",\n",
    "                 reload_from_files=True,\n",
    "                 expand_filepaths_to_save_dir=True,\n",
    "                 cuda=False,\n",
    "                 seed=1337,\n",
    "                 learning_rate=5e-4,\n",
    "                 batch_size=64,\n",
    "                 num_epochs=100,\n",
    "                 early_stopping_criteria=5,              \n",
    "                 source_embedding_size=64, \n",
    "                 target_embedding_size=64,\n",
    "                 encoding_size=64,\n",
    "                 catch_keyboard_interrupt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fec3abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NMTDataset.load_dataset_and_make_vectorizer(args.dataset_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = NMTModel(source_vocab_size=len(vectorizer.source_vocab), \n",
    "                 source_embedding_size=args.source_embedding_size, \n",
    "                 target_vocab_size=len(vectorizer.target_vocab),\n",
    "                 target_embedding_size=args.target_embedding_size, \n",
    "                 encoding_size=args.encoding_size,\n",
    "                 target_bos_index=vectorizer.target_vocab.begin_seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "344f1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2dfdfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "        \n",
    "    for batch_index, batch_dict in enumerate(batch_generator) :\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_dict['x_source'],\n",
    "                      batch_dict['x_source_length'],\n",
    "                      batch_dict['x_target'])\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = model(batch_dict['x_source'], \n",
    "                       batch_dict['x_source_length'], \n",
    "                       batch_dict['x_target'])\n",
    "\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=model, \n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8367cf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMTModel(\n",
       "  (encoder): NMTEncoder(\n",
       "    (source_embedding): Embedding(3025, 64, padding_idx=0)\n",
       "    (birnn): GRU(64, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): NMTDecoder(\n",
       "    (target_embedding): Embedding(4902, 64, padding_idx=0)\n",
       "    (gru_cell): GRUCell(192, 128)\n",
       "    (hidden_map): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (classifier): Linear(in_features=256, out_features=4902, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chencherry = bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_indices(indices, vocab, strict=True, return_string=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            break\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    if return_string:\n",
    "        return \" \".join(out)\n",
    "    else:\n",
    "        return out\n",
    "    \n",
    "class NMTSampler:\n",
    "    def __init__(self, vectorizer, model):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "    \n",
    "    def apply_to_batch(self, batch_dict):\n",
    "        self._last_batch = batch_dict\n",
    "        y_pred = self.model(x_source=batch_dict['x_source'], \n",
    "                            x_source_lengths=batch_dict['x_source_length'], \n",
    "                            target_sequence=batch_dict['x_target'])\n",
    "        self._last_batch['y_pred'] = y_pred\n",
    "        \n",
    "        attention_batched = np.stack(self.model.decoder._cached_p_attn).transpose(1, 0, 2)\n",
    "        self._last_batch['attention'] = attention_batched\n",
    "        \n",
    "    def _get_source_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['x_source'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.source_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "    \n",
    "    def _get_reference_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['y_target'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "    \n",
    "    def _get_sampled_sentence(self, index, return_string=True):\n",
    "        _, all_indices = torch.max(self._last_batch['y_pred'], dim=2)\n",
    "        sentence_indices = all_indices[index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(sentence_indices, vocab, return_string=return_string)\n",
    "\n",
    "    def get_ith_item(self, index, return_string=True):\n",
    "        output = {\"source\": self._get_source_sentence(index, return_string=return_string), \n",
    "                  \"reference\": self._get_reference_sentence(index, return_string=return_string), \n",
    "                  \"sampled\": self._get_sampled_sentence(index, return_string=return_string),\n",
    "                  \"attention\": self._last_batch['attention'][index]}\n",
    "        \n",
    "        reference = output['reference']\n",
    "        hypothesis = output['sampled']\n",
    "        \n",
    "        if not return_string:\n",
    "            reference = \" \".join(reference)\n",
    "            hypothesis = \" \".join(hypothesis)\n",
    "        \n",
    "        output['bleu-4'] = bleu_score.sentence_bleu(references=[reference],\n",
    "                                                    hypothesis=hypothesis,\n",
    "                                                    smoothing_function=chencherry.method1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb933622",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = NMTSampler(vectorizer, model)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "\n",
    "test_results = []\n",
    "for batch_dict in batch_generator:\n",
    "    sampler.apply_to_batch(batch_dict)\n",
    "    for i in range(args.batch_size):\n",
    "        test_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba96ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([r['bleu-4'] for r in test_results], bins=100);\n",
    "np.mean([r['bleu-4'] for r in test_results]), np.median([r['bleu-4'] for r in test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('val')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "batch_dict = next(batch_generator)\n",
    "\n",
    "model = model.eval().to(args.device)\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "sampler.apply_to_batch(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a52a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(args.batch_size):\n",
    "    all_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8078178",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [x for x in all_results if x['bleu-4']>0.1]\n",
    "len(top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_sentence(vectorizer, batch_dict, index):\n",
    "    indices = batch_dict['x_source'][index].cpu().data.numpy()\n",
    "    vocab = vectorizer.source_vocab\n",
    "    return sentence_from_indices(indices, vocab)\n",
    "\n",
    "def get_true_sentence(vectorizer, batch_dict, index):\n",
    "    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "    \n",
    "def get_sampled_sentence(vectorizer, batch_dict, index):\n",
    "    y_pred = model(x_source=batch_dict['x_source'], \n",
    "                   x_source_lengths=batch_dict['x_source_length'], \n",
    "                   target_sequence=batch_dict['x_target'])\n",
    "    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "\n",
    "def get_all_sentences(vectorizer, batch_dict, index):\n",
    "    return {\"source\": get_source_sentence(vectorizer, batch_dict, index), \n",
    "            \"truth\": get_true_sentence(vectorizer, batch_dict, index), \n",
    "            \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)}\n",
    "    \n",
    "def sentence_from_indices(indices, vocab, strict=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            return \" \".join(out)\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_sentences(vectorizer, batch_dict, 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbaba48",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### 샘플링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_embeddings (int): 임베딩 개수는 타깃 어휘 사전에 있는 고유한 단어의 개수이다\n",
    "            embedding_size (int): 임베딩 벡터 크기\n",
    "            rnn_hidden_size (int): RNN 은닉 상태 크기\n",
    "            bos_index(int): begin-of-sequence 인덱스\n",
    "        \"\"\"\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                             embedding_dim=embedding_size, \n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, \n",
    "                                   rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "        self._sampling_temperature = 3\n",
    "        \n",
    "    # Begin_of_sequence 인덱스 벡터 반환\n",
    "    def _init_indices(self, batch_size) :\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "    \n",
    "    # 문맥 벡터 초기화하기 위한 0벡터 반환\n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "    \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence, sample_probability=0.0):\n",
    "        if target_sequence is None:\n",
    "            sample_probability = 1.0\n",
    "        else:\n",
    "            # 가정: 첫 번째 차원은 배치 차원입니다\n",
    "            # 즉 입력은 (Batch, Seq)\n",
    "            # 시퀀스에 대해 반복해야 하므로 (Seq, Batch)로 차원을 바꿉니다\n",
    "            target_sequence = target_sequence.permute(1, 0)\n",
    "            output_sequence_size = target_sequence.size(0)\n",
    "        \n",
    "        # 주어진 인코더의 은닉 상태를 초기 은닉 상태로 사용합니다\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        # 문맥 벡터를 0으로 초기화합니다\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        # 첫 단어 y_t를 BOS로 초기화합니다\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            # 스케줄링된 샘플링 사용 여부\n",
    "            use_sample = np.random.random() < sample_probability\n",
    "            if not use_sample:\n",
    "                y_t_index = target_sequence[i]\n",
    "                \n",
    "            # 단계 1: 단어를 임베딩하고 이전 문맥과 연결합니다\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # 단계 2: GRU를 적용하고 새로운 은닉 벡터를 얻습니다\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            \n",
    "            # 단계 3: 현재 은닉 상태를 사용해 인코더의 상태를 주목합니다\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state, \n",
    "                                                           query_vector=h_t)\n",
    "            \n",
    "            # 부가 작업: 시각화를 위해 어텐션 확률을 저장합니다\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            # 단게 4: 현재 은닉 상태와 문맥 벡터를 사용해 다음 단어를 예측합니다\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            \n",
    "            \n",
    "            # 샘플링 수행\n",
    "            if use_sample:\n",
    "                p_y_t_index = F.softmax(score_for_y_t_index * self._sampling_temperature, dim=1) # 뾰족 분포 생성\n",
    "                # _, y_t_index = torch.max(p_y_t_index, 1) # 최댓값 예측, 확률이 가장 높은 단어\n",
    "                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze() # 확률에 비례하여 다항분포에서 인덱스 샘플링\n",
    "            \n",
    "            # 부가 작업: 예측 성능 점수를 기록\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        \n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0895b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(dataset_csv=\"./Data/simplest_eng_fra.csv\",\n",
    "                 vectorizer_file=\"vectorizer.json\",\n",
    "                 model_state_file=\"model8.pth\",\n",
    "                 save_dir=\"./\",\n",
    "                 reload_from_files=False,\n",
    "                 expand_filepaths_to_save_dir=True,\n",
    "                 cuda=True,\n",
    "                 seed=1337,\n",
    "                 learning_rate=5e-4,\n",
    "                 batch_size=32,\n",
    "                 num_epochs=100,\n",
    "                 early_stopping_criteria=5,              \n",
    "                 source_embedding_size=24, \n",
    "                 target_embedding_size=24,\n",
    "                 encoding_size=32,\n",
    "                 catch_keyboard_interrupt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NMTDataset.load_dataset_and_make_vectorizer(args.dataset_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = NMTModel(source_vocab_size=len(vectorizer.source_vocab), \n",
    "                 source_embedding_size=args.source_embedding_size, \n",
    "                 target_vocab_size=len(vectorizer.target_vocab),\n",
    "                 target_embedding_size=args.target_embedding_size, \n",
    "                 encoding_size=args.encoding_size,\n",
    "                 target_bos_index=vectorizer.target_vocab.begin_seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    sample_probability = (20 + epoch_index) / args.num_epochs\n",
    "\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_dict['x_source'],\n",
    "                      batch_dict['x_source_length'],\n",
    "                      batch_dict['x_target'],\n",
    "                      sample_probability = sample_probability)\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = model(batch_dict['x_source'], \n",
    "                       batch_dict['x_source_length'], \n",
    "                       batch_dict['x_target'],\n",
    "                       sample_probability=sample_probability)\n",
    "\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=model, \n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa179535",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('val')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "batch_dict = next(batch_generator)\n",
    "\n",
    "model = model.eval().to(args.device)\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "sampler.apply_to_batch(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea677719",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(args.batch_size):\n",
    "    all_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [x for x in all_results if x['bleu-4']>0.5]\n",
    "len(top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_sentences(vectorizer, batch_dict, 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a05800",
   "metadata": {},
   "source": [
    "* 샘플링된 것의 각 미니배치는 텐서 4개로 구성\n",
    "    - 소스 시퀀스의 정수 행렬 1 / 타깃 시퀀스의 정수 행렬 2 (하나는 어긋난 시퀀스) / 소스 시퀀스 길이의 정수 벡터 1\n",
    "    \n",
    "* 두 버전의 모델이 타깃 시퀀스를 다루는 방법이 다르다.\n",
    "    - 샘플링 X : 제공된 타깃 시퀀스를 타임 스텝마다 디코더의 입력으로 사용\n",
    "    - 샘플링 O : 스케줄링된 샘플링 사용해 모델이 자체 예측을 만들어 디코더의 입력으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6eb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a993d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
