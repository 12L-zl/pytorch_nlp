{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e4b8d7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp311-cp311-win_amd64.whl (172.3 MB)\n",
      "     ---------------------------------------- 0.0/172.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.5/172.3 MB 16.8 MB/s eta 0:00:11\n",
      "     --------------------------------------- 1.0/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "     --------------------------------------- 1.5/172.3 MB 12.2 MB/s eta 0:00:14\n",
      "     --------------------------------------- 2.1/172.3 MB 11.9 MB/s eta 0:00:15\n",
      "      -------------------------------------- 2.5/172.3 MB 11.4 MB/s eta 0:00:15\n",
      "      -------------------------------------- 3.0/172.3 MB 11.4 MB/s eta 0:00:15\n",
      "      -------------------------------------- 3.6/172.3 MB 11.4 MB/s eta 0:00:15\n",
      "      -------------------------------------- 4.0/172.3 MB 11.1 MB/s eta 0:00:16\n",
      "      -------------------------------------- 4.4/172.3 MB 10.7 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 4.9/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 5.4/172.3 MB 10.9 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 5.9/172.3 MB 10.7 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 6.4/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 6.9/172.3 MB 11.0 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 7.4/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "     - ------------------------------------- 8.0/172.3 MB 11.1 MB/s eta 0:00:15\n",
      "     - ------------------------------------- 8.5/172.3 MB 11.1 MB/s eta 0:00:15\n",
      "     - ------------------------------------- 8.8/172.3 MB 10.8 MB/s eta 0:00:16\n",
      "     -- ------------------------------------ 9.3/172.3 MB 10.9 MB/s eta 0:00:16\n",
      "     -- ------------------------------------ 9.9/172.3 MB 10.9 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 10.3/172.3 MB 10.7 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 10.9/172.3 MB 10.7 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 11.4/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 11.9/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 12.5/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 12.8/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 13.5/172.3 MB 10.6 MB/s eta 0:00:16\n",
      "     --- ---------------------------------- 13.9/172.3 MB 10.6 MB/s eta 0:00:16\n",
      "     --- ---------------------------------- 14.4/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 14.8/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 15.3/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 15.9/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 16.6/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 17.1/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 17.6/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 18.1/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 18.9/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 19.3/172.3 MB 10.7 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 19.4/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 19.8/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 20.4/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 21.1/172.3 MB 10.4 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 21.8/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 22.3/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 22.8/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 23.3/172.3 MB 10.2 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 23.9/172.3 MB 10.4 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 24.4/172.3 MB 10.6 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 24.9/172.3 MB 10.6 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 25.4/172.3 MB 10.6 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 26.0/172.3 MB 10.6 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 26.5/172.3 MB 10.7 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 27.0/172.3 MB 10.9 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 27.5/172.3 MB 10.9 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 28.0/172.3 MB 10.7 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 28.5/172.3 MB 10.7 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 29.1/172.3 MB 10.7 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 29.6/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 30.1/172.3 MB 11.5 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 30.7/172.3 MB 11.5 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 31.2/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 31.7/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 32.2/172.3 MB 11.5 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 32.8/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 33.3/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 33.8/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 34.3/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 34.8/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 35.3/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 35.8/172.3 MB 11.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 36.3/172.3 MB 11.5 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 36.7/172.3 MB 11.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 37.2/172.3 MB 11.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 37.7/172.3 MB 11.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 38.2/172.3 MB 11.3 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 38.8/172.3 MB 11.3 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 39.3/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 39.8/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 40.4/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 40.9/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 40.9/172.3 MB 10.9 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 41.3/172.3 MB 10.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 41.9/172.3 MB 10.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 42.4/172.3 MB 10.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 43.0/172.3 MB 10.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 43.5/172.3 MB 10.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 44.0/172.3 MB 10.7 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 44.6/172.3 MB 10.7 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 45.1/172.3 MB 10.7 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 45.7/172.3 MB 10.7 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 46.4/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 46.9/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 47.4/172.3 MB 10.7 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 48.0/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 48.5/172.3 MB 11.1 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 49.2/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ---------- --------------------------- 49.7/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ----------- -------------------------- 50.4/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ----------- -------------------------- 50.9/172.3 MB 10.9 MB/s eta 0:00:12\n",
      "     ----------- -------------------------- 51.4/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 51.9/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 52.6/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 53.4/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 53.9/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 54.6/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 55.3/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 55.9/172.3 MB 11.7 MB/s eta 0:00:10\n",
      "     ------------ ------------------------- 56.5/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 57.3/172.3 MB 11.5 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 57.9/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------ ------------------------- 58.4/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 59.1/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 59.6/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 60.2/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 60.7/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 61.2/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 61.8/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 62.3/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 62.9/172.3 MB 11.7 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 63.4/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 63.9/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 64.4/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 65.0/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 65.5/172.3 MB 11.3 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 65.9/172.3 MB 11.3 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 66.5/172.3 MB 11.3 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 66.9/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 67.5/172.3 MB 11.1 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 68.0/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 68.2/172.3 MB 11.5 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 68.2/172.3 MB 10.6 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 68.8/172.3 MB 10.6 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 69.4/172.3 MB 10.6 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 69.8/172.3 MB 10.4 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 70.2/172.3 MB 10.2 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 70.7/172.3 MB 10.1 MB/s eta 0:00:11\n",
      "     --------------- ---------------------- 71.2/172.3 MB 10.1 MB/s eta 0:00:11\n",
      "     --------------- ---------------------- 71.7/172.3 MB 10.1 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 72.3/172.3 MB 10.2 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 72.3/172.3 MB 10.2 MB/s eta 0:00:10\n",
      "     --------------- ---------------------- 72.3/172.3 MB 10.2 MB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 72.6/172.3 MB 9.1 MB/s eta 0:00:11\n",
      "     ---------------- ---------------------- 73.0/172.3 MB 9.0 MB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 73.7/172.3 MB 9.1 MB/s eta 0:00:11\n",
      "     ---------------- ---------------------- 74.1/172.3 MB 8.8 MB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 74.5/172.3 MB 8.8 MB/s eta 0:00:12\n",
      "     ----------------- --------------------- 75.2/172.3 MB 8.8 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 75.8/172.3 MB 8.8 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 76.2/172.3 MB 8.8 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 76.7/172.3 MB 8.7 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 77.5/172.3 MB 8.8 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 78.0/172.3 MB 8.8 MB/s eta 0:00:11\n",
      "     ----------------- --------------------- 78.5/172.3 MB 9.5 MB/s eta 0:00:10\n",
      "     ----------------- --------------------- 79.1/172.3 MB 9.5 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 79.6/172.3 MB 9.5 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 80.2/172.3 MB 9.6 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 80.6/172.3 MB 9.6 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 81.0/172.3 MB 9.6 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 81.4/172.3 MB 9.5 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 81.9/172.3 MB 9.6 MB/s eta 0:00:10\n",
      "     ------------------ -------------------- 82.5/172.3 MB 9.6 MB/s eta 0:00:10\n",
      "     ------------------ ------------------- 83.2/172.3 MB 10.9 MB/s eta 0:00:09\n",
      "     ------------------ ------------------- 83.7/172.3 MB 10.9 MB/s eta 0:00:09\n",
      "     ------------------ ------------------- 84.3/172.3 MB 11.1 MB/s eta 0:00:08\n",
      "     ------------------ ------------------- 84.8/172.3 MB 11.1 MB/s eta 0:00:08\n",
      "     ------------------ ------------------- 85.5/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 86.2/172.3 MB 11.1 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 86.7/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 87.4/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 88.1/172.3 MB 11.1 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 88.7/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 88.9/172.3 MB 10.9 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 89.6/172.3 MB 10.9 MB/s eta 0:00:08\n",
      "     ------------------- ------------------ 90.3/172.3 MB 10.7 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 90.8/172.3 MB 11.1 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 91.4/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 91.9/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 92.4/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 93.0/172.3 MB 11.3 MB/s eta 0:00:08\n",
      "     -------------------- ----------------- 93.6/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     -------------------- ----------------- 94.2/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     -------------------- ----------------- 95.0/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 95.5/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 96.1/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 96.6/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 97.1/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 97.7/172.3 MB 11.5 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 98.2/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 98.7/172.3 MB 11.1 MB/s eta 0:00:07\n",
      "     --------------------- ---------------- 99.3/172.3 MB 11.7 MB/s eta 0:00:07\n",
      "     ---------------------- --------------- 99.8/172.3 MB 11.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 100.3/172.3 MB 11.5 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 100.9/172.3 MB 11.5 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 101.4/172.3 MB 11.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 101.9/172.3 MB 11.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 102.5/172.3 MB 11.7 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 103.0/172.3 MB 11.7 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 103.4/172.3 MB 11.7 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 103.8/172.3 MB 11.5 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 104.4/172.3 MB 11.3 MB/s eta 0:00:07\n",
      "     ---------------------- -------------- 104.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 105.5/172.3 MB 11.5 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.0/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.4/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 106.9/172.3 MB 11.3 MB/s eta 0:00:06\n",
      "     ----------------------- -------------- 107.2/172.3 MB 8.7 MB/s eta 0:00:08\n",
      "     ----------------------- -------------- 107.8/172.3 MB 8.7 MB/s eta 0:00:08\n",
      "     ----------------------- -------------- 108.5/172.3 MB 8.8 MB/s eta 0:00:08\n",
      "     ------------------------ ------------- 109.0/172.3 MB 8.8 MB/s eta 0:00:08\n",
      "     ------------------------ ------------- 109.6/172.3 MB 8.8 MB/s eta 0:00:08\n",
      "     ------------------------ ------------- 110.1/172.3 MB 8.8 MB/s eta 0:00:08\n",
      "     ------------------------ ------------- 110.6/172.3 MB 8.8 MB/s eta 0:00:07\n",
      "     ------------------------ ------------- 111.2/172.3 MB 8.7 MB/s eta 0:00:08\n",
      "     ------------------------ ------------- 111.8/172.3 MB 8.7 MB/s eta 0:00:07\n",
      "     ------------------------ ------------- 112.3/172.3 MB 8.7 MB/s eta 0:00:07\n",
      "     ------------------------ ------------- 112.9/172.3 MB 8.7 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 113.4/172.3 MB 8.7 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 113.9/172.3 MB 9.1 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 114.4/172.3 MB 9.0 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 115.2/172.3 MB 9.1 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 115.8/172.3 MB 9.1 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 116.3/172.3 MB 9.1 MB/s eta 0:00:07\n",
      "     ------------------------- ------------ 117.0/172.3 MB 9.1 MB/s eta 0:00:07\n",
      "     ------------------------- ----------- 117.5/172.3 MB 11.9 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 118.1/172.3 MB 11.9 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 118.8/172.3 MB 11.9 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 119.3/172.3 MB 11.9 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 119.8/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 120.6/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 121.1/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 121.6/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 122.2/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 122.7/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 123.2/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 123.8/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 124.3/172.3 MB 11.9 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 124.9/172.3 MB 11.7 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 125.6/172.3 MB 11.9 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 126.1/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 126.6/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 127.1/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 127.7/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 128.2/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 128.7/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 129.3/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 129.9/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 130.6/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 131.2/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 131.9/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 132.4/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 133.1/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 133.7/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 134.1/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 134.7/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 135.3/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 135.8/172.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 136.3/172.3 MB 11.3 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 137.0/172.3 MB 11.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 137.6/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 138.1/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 138.6/172.3 MB 11.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 139.2/172.3 MB 11.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 139.7/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 140.2/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 140.8/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 141.3/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 141.9/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 142.4/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 142.9/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 143.5/172.3 MB 11.5 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 143.9/172.3 MB 11.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 144.5/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 145.0/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 145.5/172.3 MB 11.3 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 146.0/172.3 MB 11.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 146.6/172.3 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 147.1/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 147.7/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 148.2/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 148.8/172.3 MB 11.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 149.5/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 150.0/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 150.6/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 151.1/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 151.6/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 152.2/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 152.7/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 153.2/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 154.0/172.3 MB 11.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 154.6/172.3 MB 11.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 155.4/172.3 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 155.9/172.3 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 156.4/172.3 MB 11.9 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 157.1/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 157.7/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 158.4/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 158.9/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 159.4/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 160.0/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 160.6/172.3 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 161.1/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 161.8/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 162.4/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 162.9/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 163.4/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 163.9/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 164.4/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 164.9/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 165.4/172.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 166.1/172.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 166.6/172.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 167.2/172.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  167.7/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  168.2/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  168.9/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  169.7/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  170.2/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  170.8/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  171.5/172.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.0/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  172.3/172.3 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 172.3/172.3 MB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\knuyh\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71285f0f",
   "metadata": {},
   "source": [
    "퍼셉트론 : 가장 간단한 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87481d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module) :\n",
    "    # 퍼셉트론은 하나의 선형 층\n",
    "    def __init__(self, input_dim) :  # input_dim : 입력 특성 크기\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x_in) :\n",
    "        '''\n",
    "        퍼셉트론의 정방향 계산\n",
    "        \n",
    "        x_in(torch.Tensor) : 입력 데이터 텐서\n",
    "        x_in.shape : (batch, num_features)  # input_dim = num_features\n",
    "                       데이터, 변수\n",
    "        반환값 : 결과 텐서, tensor.shape : (batch, )\n",
    "        '''\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze() # 차원이 1인 차원 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35685ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5799, 0.5799, 0.5799], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Perceptron(4)\n",
    "x_in = torch.Tensor(3, 4)\n",
    "x_in.shape\n",
    "a.forward(x_in) # (3, 1) # __init__ fc1이 input_dim, 2이면 (3, 2)반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e908b281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0286e-38, 1.0653e-38, 1.0194e-38, 8.4490e-39],\n",
       "        [1.0469e-38, 9.3674e-39, 9.9184e-39, 8.7245e-39],\n",
       "        [9.2755e-39, 8.9082e-39, 9.9184e-39, 8.4490e-39]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0245b7",
   "metadata": {},
   "source": [
    "활성화 함수 ; 비선형 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762b199",
   "metadata": {},
   "source": [
    "손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60708e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6011, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 평균 제곱 오차 손실(MSE)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True) # 예측\n",
    "targets = torch.randn(3, 5) # 타깃값\n",
    "loss = mse_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae4d0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9061, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 범주형 크로스 엔트로피 손실\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.tensor([1, 0, 3], dtype = torch.int64) # 정수로 하는 이유 -> 범주형을 인덱스\n",
    "loss = ce_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b7ea820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2631],\n",
      "        [0.4016],\n",
      "        [0.3129],\n",
      "        [0.5826]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.9711, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 이진 크로스 엔트로피 손실\n",
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\n",
    "loss = bce_loss(probabilities, targets)\n",
    "print(probabilities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce17b54",
   "metadata": {},
   "source": [
    "지도학습 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cb329df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_CENTER = (3, 3)\n",
    "RIGHT_CENTER = (3, -2)\n",
    "\n",
    "import numpy as np\n",
    "# 데이터 준비함수\n",
    "# 데이터 포인트가 두 분포 중 어디에 속하는지 구별\n",
    "def get_toy_data(batch_size, left_center=LEFT_CENTER, right_center=RIGHT_CENTER):\n",
    "    x_data = []\n",
    "    y_targets = np.zeros(batch_size)\n",
    "    for batch_i in range(batch_size):\n",
    "        if np.random.random() > 0.5:\n",
    "            x_data.append(np.random.normal(loc=left_center))\n",
    "        else:\n",
    "            x_data.append(np.random.normal(loc=right_center))\n",
    "            y_targets[batch_i] = 1\n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b4bf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam 옵티마이저\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 2\n",
    "lr = 0.001\n",
    "perceptron = Perceptron(input_dim = input_dim)\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(params = perceptron.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c12b9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "n_epochs = 12\n",
    "n_batches = 5\n",
    "\n",
    "# 각 에포크는 전체 훈련 데이터 사용\n",
    "for i in range(n_epochs) :\n",
    "    # 내부 반복은 데이터셋에 있는 배치에 대해 수행\n",
    "    for j in range(n_batches) :\n",
    "        \n",
    "        # 데이터 가져오기\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "        \n",
    "        # 그레이디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델의 정방향 계산 수행\n",
    "        y_pred = perceptron(x_data).squeeze()\n",
    "        \n",
    "        # 최적화하려는 손실 계산\n",
    "        loss = bce_loss(y_pred, y_target)\n",
    "        \n",
    "        # 손실 신호 거꾸로 전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저로 파라미터에 그레이디언트 업데이트\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c51def46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fb8c7",
   "metadata": {},
   "source": [
    "예제 : 레스토랑 리뷰 감성 분류\n",
    "1. 옐프 리뷰 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c23902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cd14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv=r\"C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치로 배우는 자연어처리\\raw_train.csv\",\n",
    "    raw_test_dataset_csv=r\"C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치로 배우는 자연어처리\\raw_test.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.3,\n",
    "    output_munged_csv=r\"C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치로 배우는 자연어처리\\reviews_with_splits_full.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da4fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터를 읽습니다\n",
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
    "train_reviews = train_reviews[~pd.isnull(train_reviews.review)]\n",
    "test_reviews = pd.read_csv(args.raw_test_dataset_csv, header=None, names=['rating', 'review'])\n",
    "test_reviews = test_reviews[~pd.isnull(test_reviews.review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37542092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유 클래스\n",
    "set(train_reviews.rating) # 1, 2 점만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80e50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 검증, 테스트를 만들기 위해 별점을 기준으로 나눕니다\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    \n",
    "'''\n",
    "[row]\n",
    "\n",
    "rating                                                    1\n",
    "review    Unfortunately, the frustration of being Dr. Go...\n",
    "Name: 0, dtype: object\n",
    "rating                                                    2\n",
    "review    Been going to Dr. Goldberg for over 10 years. ...\n",
    "Name: 1, dtype: object\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "348bf685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rating': 1,\n",
       "  'review': 'Just recently went there and ordered \\\\\"Super Nachos\\\\\" Didn\\'t get anything close to super nachos. Side order of guacomole was super runny; order of nachos was way too small for the price paid for it and no refried beans were on the nachos; barely any meat or cheese also. Went back for a refund, some lady who claimed to be the manager  named \\\\\"Virginia\\\\\" denied my request for another menu item comparable to the price I paid for these lousy super nachos or a refund. As a returning customer, I\\'m appalled at such service. I liked going there, because the location was convenient from my house and the food and service was always good up until now. I will not go there again !!! Rude attitude and lousy customer service! F+!',\n",
       "  'split': 'train'},\n",
       " {'rating': 1,\n",
       "  'review': 'Slow wait staff, simple food yet overpriced. I work downtown and refuse to go here after 2 bad experiences.',\n",
       "  'split': 'train'},\n",
       " {'rating': 1,\n",
       "  'review': 'Not worth the money! My husband took his parents here after a couple reviews from family saying it was amazing, its not. The food was flavorless and the portions were a little small. If you want good Chinese food drive a little further to Good China on Thomas.',\n",
       "  'split': 'train'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_rating[1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d4f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 데이터를 만듭니다.\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):  # _ : 별점 (1, 2, 0, 3, 4, 100), item_list :\n",
    "\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    #print(n_total)  # 0 280000 280000 0 0 0\n",
    "    n_train = int(args.train_proportion * n_total) # 0.7 *\n",
    "    n_val = int(args.val_proportion * n_total) # 0.3 *\n",
    "    \n",
    "    # 데이터 포인터에 분할 속성을 추가합니다\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    \n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "\n",
    "    # 최종 리스트에 추가합니다\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c8c1875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rating': 1,\n",
       "  'review': 'Just recently went there and ordered \\\\\"Super Nachos\\\\\" Didn\\'t get anything close to super nachos. Side order of guacomole was super runny; order of nachos was way too small for the price paid for it and no refried beans were on the nachos; barely any meat or cheese also. Went back for a refund, some lady who claimed to be the manager  named \\\\\"Virginia\\\\\" denied my request for another menu item comparable to the price I paid for these lousy super nachos or a refund. As a returning customer, I\\'m appalled at such service. I liked going there, because the location was convenient from my house and the food and service was always good up until now. I will not go there again !!! Rude attitude and lousy customer service! F+!',\n",
       "  'split': 'train'},\n",
       " {'rating': 1,\n",
       "  'review': 'Slow wait staff, simple food yet overpriced. I work downtown and refuse to go here after 2 bad experiences.',\n",
       "  'split': 'train'},\n",
       " {'rating': 1,\n",
       "  'review': 'Not worth the money! My husband took his parents here after a couple reviews from family saying it was amazing, its not. The food was flavorless and the portions were a little small. If you want good Chinese food drive a little further to Good China on Thomas.',\n",
       "  'split': 'train'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6b28503",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in test_reviews.iterrows():\n",
    "    row_dict = row.to_dict()\n",
    "    row_dict['split'] = 'test'\n",
    "    final_list.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b4d8087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Just recently went there and ordered \\\"Super N...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Slow wait staff, simple food yet overpriced. I...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Not worth the money! My husband took his paren...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Front desk service was awful. The two people b...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Went after it been open a couple weeks. They m...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597995</th>\n",
       "      <td>1</td>\n",
       "      <td>After spending 80 bucks per person ($ 20 for e...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597996</th>\n",
       "      <td>2</td>\n",
       "      <td>Stellar! One of my favorite places to eat in a...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597997</th>\n",
       "      <td>1</td>\n",
       "      <td>We stopped by here for a dessert after Fuddruc...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597998</th>\n",
       "      <td>1</td>\n",
       "      <td>Wait staff was attentive but the food was very...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597999</th>\n",
       "      <td>2</td>\n",
       "      <td>I like the Light Rail.  Just wish it went more...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>598000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                             review  split\n",
       "0            1  Just recently went there and ordered \\\"Super N...  train\n",
       "1            1  Slow wait staff, simple food yet overpriced. I...  train\n",
       "2            1  Not worth the money! My husband took his paren...  train\n",
       "3            1  Front desk service was awful. The two people b...  train\n",
       "4            1  Went after it been open a couple weeks. They m...  train\n",
       "...        ...                                                ...    ...\n",
       "597995       1  After spending 80 bucks per person ($ 20 for e...   test\n",
       "597996       2  Stellar! One of my favorite places to eat in a...   test\n",
       "597997       1  We stopped by here for a dessert after Fuddruc...   test\n",
       "597998       1  Wait staff was attentive but the food was very...   test\n",
       "597999       2  I like the Light Rail.  Just wish it went more...   test\n",
       "\n",
       "[598000 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분할 데이터를 데이터 프레임으로 만듭니다\n",
    "final_reviews = pd.DataFrame(final_list)\n",
    "final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa565780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    392000\n",
       "val      168000\n",
       "test      38000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2713cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rating, review, split]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews[pd.isnull(final_reviews.review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80f69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰를 전처리합니다\n",
    "def preprocess_text(text):\n",
    "    if type(text) == float:\n",
    "        print(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "958a7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews['rating'] = final_reviews.rating.apply({1: 'negative', 2: 'positive'}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d91c5771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>just recently went there and ordered super nac...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>slow wait staff , simple food yet overpriced ....</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>not worth the money ! my husband took his pare...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>front desk service was awful . the two people ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>went after it been open a couple weeks . they ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  just recently went there and ordered super nac...  train\n",
       "1  negative  slow wait staff , simple food yet overpriced ....  train\n",
       "2  negative  not worth the money ! my husband took his pare...  train\n",
       "3  negative  front desk service was awful . the two people ...  train\n",
       "4  negative  went after it been open a couple weeks . they ...  train"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5eef53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c5ef3",
   "metadata": {},
   "source": [
    "2. 파이토치 데이터셋 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0203bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReviewDataset 클래스는 데이터셋이 최소한으로 정제되고 3개로 나뉘었다고 가정\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset) :\n",
    "    def __init__(self, review_df, vectorizer) :\n",
    "        '''\n",
    "        매개변수\n",
    "        review_df : DF, 데이터셋\n",
    "        vectorizer : ReviewVectorizer 객체\n",
    "        '''\n",
    "        self.review_df = review_df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)        \n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train' : (self.train_df, self.train_size),\n",
    "                            'val' : (self.val_df, self.val_size),\n",
    "                            'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv) :\n",
    "        # 데이터셋 로드하고 새로운 ReviewVectorizer 객체 생성\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))  # ReviewDataset 인스턴스 반환\n",
    "        # ReviewVectorizer : 리뷰 텍스트를 수치 벡터로 변환 /데이터프레임 기반\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\" 파일에서 ReviewVectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 ReviewVectorizer 객체의 위치\n",
    "        반환값:\n",
    "            ReviewVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\" ReviewVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "    \n",
    "    def get_vectorizer(self) :\n",
    "        return self._vectorizer # ReviewVectorizer 객체 반환\n",
    "    \n",
    "    def set_split(self, split='train') :\n",
    "        # 데이터프레임에 있는 열 사용해 분할 세트 선택\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    # 파이토치 데이터셋 주요 진입 메서드\n",
    "    def __len__(self) : # 데이터셋 크기 리턴\n",
    "        return self._target_size\n",
    "    def __getitem__(self, index) : # i번째 샘플 찾기\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self.vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        \n",
    "        return {'x_data' : review_vector, \n",
    "               'y_target' : rating_index}\n",
    "    \n",
    "    \n",
    "    def get_num_batches(self, batch_size) :\n",
    "        # 배치 크기 주어지면, 배치 개수 반환\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83891a4",
   "metadata": {},
   "source": [
    "3. Vocabulary, Vetorizer, DataLoader 클래스  \n",
    "텍스트를 벡터의 미니배치로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755109b",
   "metadata": {},
   "source": [
    "* Vocabulary  \n",
    "첫번째 단계, 토큰을 정수로 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e6bb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰과 정수 매핑 관리\n",
    "class Vocabulary(object) :\n",
    "    \n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = '<UNK>') :\n",
    "        '''\n",
    "        token_to_idx (dict) : 기존 토큰-인덱스 매핑 dict\n",
    "        add_unk (bool) : UNK 토큰을 추가할지 지정하는 플래그\n",
    "        unk_token (str) : 추가할 UNK 토큰\n",
    "        '''\n",
    "        if token_to_idx is None :\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx : token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk :\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    \n",
    "    # 직렬화할 수 있는 dict 반환\n",
    "    def to_serializable(self) :\n",
    "        return {'token_to_idx' : self._token_to_idx,\n",
    "               'add_unk' : self._add_unk,\n",
    "               'unk_token' : self._unk_token}\n",
    "    \n",
    "    # 직렬화된 dict에서 Vocabulary 객체 생성\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents) :\n",
    "        return cls(**contents)\n",
    "    \n",
    "    # 토큰 기반 매핑 dict 업데이트\n",
    "    def add_token(self, token) : # 새로운 토큰 추가\n",
    "        if token in self._token_to_idx :   #token : Vocabulary 추가할 토큰\n",
    "            index = self._token_to_idx[token]\n",
    "        else :\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    # 토큰 리스트 Vocabulary에 추가\n",
    "    def add_many(self, tokens):\n",
    "        '''\n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        '''\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    # 토큰에 대응하는 인덱스 추출, 없으면 UNK 반환\n",
    "    def lookup_token(self, token) : \n",
    "        if self.add_unk :\n",
    "            return self._token_to_idx.get(token, self.unk_index) # token : 찾을 토큰\n",
    "        else : # UNK일 때\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    # 인덱스에 해당하는 토큰 반환\n",
    "    def lookup_index(self, index) :\n",
    "        if index not in self._idx_to_token :\n",
    "            raise KeyError('Vocabulary에 인덱스(%d)가 없다.' % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self) :\n",
    "        return '<Vocabulary(size=%d)>' % len(self)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae557a88",
   "metadata": {},
   "source": [
    "* Vectorizer  \n",
    "두 번째 단계, 입력 데이터 포인트의 토큰을 순회하며 각 토큰을 정수로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81eee6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30d46f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 수치 벡터로 변환\n",
    "class ReviewVectorizer(object) :\n",
    "    def __init__ (self, review_vocab, rating_vocab) :\n",
    "        self.review_vocab = review_vocab # 단어를 정수에 매핑하는 Vocabulary\n",
    "        self.rating_vocab = rating_vocab # 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        \n",
    "    # 리뷰에 대한 one-hot vector 생성\n",
    "    def vectorize(self, review) :\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(' ') :\n",
    "            if token not in string.punctuation :\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    # 데이터셋 데이터프레임에서 Vectorizer 객체 생성\n",
    "    def from_dataframe(cls, review_df, cutoff=25) : # 빈도 기반 필터링 설정값\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_voacb = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # 점수 추가\n",
    "        for rating in sorted(set(review_df.rating)) :\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        # count > cutoff인 단어 추가\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review :\n",
    "            for word in review.split(' ') :\n",
    "                if word not in string.punctuation :\n",
    "                    word_counts[word] += 1\n",
    "                    \n",
    "        for word, count in word_counts.items() :\n",
    "            if count > cutoff :\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab) # ReviewVectorizer 객체 반환\n",
    "    \n",
    "    @classmethod\n",
    "    # 직렬화된 dict에서 ReviewVectorizer 객체 생성\n",
    "    def from_serializable(cls, contents) :\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab) # ReveiwVectorizer 클래스 객체 반환\n",
    "    \n",
    "    # 캐싱\n",
    "    def to_serializable(self) :\n",
    "        return {'review_vocab' : self.review_vocab.to_serializable(),\n",
    "               'rating_vocab' : self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd49456",
   "metadata": {},
   "source": [
    "* DataLoader  \n",
    "마지막 단계, 벡터로 변환한 데이터 포인트 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40d6028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 미니배치 생성하기\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu') :\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader :\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items() :\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aece4f7",
   "metadata": {},
   "source": [
    "4. 퍼셉트론 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "342cb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ReviewClassifier(nn.Module) :\n",
    "    def __init__(self, num_features) :\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = num_features, out_features=1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid = False) :\n",
    "        # 크로스 엔트로피 손실 사용하려면 False로 지정\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid :\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb48248",
   "metadata": {},
   "source": [
    "5. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "088eb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv=r\"C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치로 배우는 자연어처리\\reviews_with_splits_lite.csv\",\n",
    "    save_dir=r\"C:\\Users\\knuyh\\Desktop\\민지\\스터디\\파이토치로 배우는 자연어처리\",\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # 모델 하이퍼파라미터 없음\n",
    "    # 훈련 하이퍼파라미터\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0247d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_train_state(args) :\n",
    "    return {'epoch_index' : 0,\n",
    "           'train_loss' : [],\n",
    "           'train_acc' : [],\n",
    "           'val_loss' : [],\n",
    "           'val_acc' : [],\n",
    "           'test_loss' : -1,\n",
    "           'test_acc' : -1}\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available() : \n",
    "    args.cuda = False\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ac8d446",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rating_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터셋과 Vectorizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ReviewDataset\u001b[38;5;241m.\u001b[39mload_dataset_and_make_vectorizer(args\u001b[38;5;241m.\u001b[39mreview_csv)                                                            \n\u001b[0;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_vectorizer()\n",
      "Cell \u001b[1;32mIn[87], line 31\u001b[0m, in \u001b[0;36mReviewDataset.load_dataset_and_make_vectorizer\u001b[1;34m(cls, review_csv)\u001b[0m\n\u001b[0;32m     29\u001b[0m review_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(review_csv)\n\u001b[0;32m     30\u001b[0m train_review_df \u001b[38;5;241m=\u001b[39m review_df[review_df\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(review_df, ReviewVectorizer\u001b[38;5;241m.\u001b[39mfrom_dataframe(train_review_df))\n",
      "Cell \u001b[1;32mIn[90], line 24\u001b[0m, in \u001b[0;36mReviewVectorizer.from_dataframe\u001b[1;34m(cls, review_df, cutoff)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 점수 추가\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rating \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(review_df\u001b[38;5;241m.\u001b[39mrating)) :\n\u001b[1;32m---> 24\u001b[0m     rating_vocab\u001b[38;5;241m.\u001b[39madd_token(rating)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# count > cutoff인 단어 추가\u001b[39;00m\n\u001b[0;32m     27\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m Counter()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rating_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 Vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)                                                            \n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f153f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "classifier = ReviewClassifier(num_features = len(vectorizer.review_vocab))\n",
    "# classifier = classifier.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 옵티마이저\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer . optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 반복\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # 훈련 세트에 대한 순회\n",
    "\n",
    "    # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "        # --------------------------------------\n",
    "        # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 단계 2. 출력을 계산합니다\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # 단계 3. 손실을 계산합니다\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "        loss.backward()\n",
    "\n",
    "        # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # 정확도를 계산합니다\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval() # 모델의 파라미터 수정하지 못하게 하고, 드롭아웃 비활성화\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20b9de",
   "metadata": {},
   "source": [
    "6. 평가, 추론, 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86115e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eaa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터 포인트 추론하여 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b35aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "                        # 훈련된 모델               클래스 나눌 결정 경계\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1)) \n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80615ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed373c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 분석\n",
    "# 퍼셉트론의 가중치는 어휘 사전의 한 단어와 정확하게 대응하므로 쉽게 확인 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27083b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# 긍정적인 상위 20개 단어\n",
    "print(\"긍정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# 부정적인 상위 20개 단어\n",
    "print(\"부정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671234aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
