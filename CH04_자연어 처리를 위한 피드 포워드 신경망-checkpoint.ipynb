{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59a98b1",
   "metadata": {},
   "source": [
    "### 4.1 다층 퍼셉트론 (MLP)\n",
    "\n",
    "* 파이토치로 MLP 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0e8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9ae802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False): # crossEntropy 사용하려면 False\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca68732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2 # 한 번에 입력할 샘플 개수\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "\n",
    "# 모델 생성\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de923f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"타입: {}\".format(x.type()))\n",
    "    print(\"크기: {}\".format(x.shape))\n",
    "    print(\"값: \\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9d52f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 3])\n",
      "값: \n",
      "tensor([[0.0977, 0.5046, 0.1484],\n",
      "        [0.2781, 0.1258, 0.5622]])\n"
     ]
    }
   ],
   "source": [
    "# random 입력\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba750050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 4])\n",
      "값: \n",
      "tensor([[ 0.0324,  0.2516,  0.0643,  0.0892],\n",
      "        [-0.1216,  0.3439, -0.0473,  0.1324]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030beb9",
   "metadata": {},
   "source": [
    "### 4.2 예제 : MLP로 성씨 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1555d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3789ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token) :\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32fb72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab # 문자를 정수에 매핑\n",
    "        self.nationality_vocab = nationality_vocab # 국적을 정수에 매핑\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5328e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 클래스 가중치\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    def __getitem__(self, index) :\n",
    "        row = self._target_df.iloc[index]\n",
    "        surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname' : surname_vector,\n",
    "               'y_nationality' : nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size) :\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bf65a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8c64b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성씨 분류를 위한 다층 퍼셉트론\n",
    "class SurnameClassifier(nn.Module) :\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim) :\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ebffe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc1fe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"./Data/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"./surname_mlp\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    hidden_dim=300,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    # 실행 옵션\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d067e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3158f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
    "                               hidden_dim=args.hidden_dim, \n",
    "                               output_dim=len(vectorizer.nationality_vocab))\n",
    "\n",
    "# classifier = classifier.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "756deaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92023bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c737d1edaf44b1aaf1ba7160b5d8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4c07210800438aa15c7550dd7472b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f87f2d8b7547cbaee06a3ba03ae6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 반복\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size), \n",
    "                               position=1, \n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size), \n",
    "                             position=1, \n",
    "                             leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "\n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           #device=args.device\n",
    "                                          )\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ebea604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "#classifier = classifier.to(args.device)\n",
    "#dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71a937ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 1.8285559988021853;\n",
      "테스트 정확도: 47.56249999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2601911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9b34e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류하려는 성씨를 입력하세요: McMahan\n",
      "McMahan -> Irish (p=0.39)\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"분류하려는 성씨를 입력하세요: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0df8c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Irish'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최상위 K개 예측\n",
    "vectorizer.nationality_vocab.lookup_index(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f9c3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # 반환되는 크기는 (1,k)입니다\n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': prob_value})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "203f5396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류하려는 성씨를 입력하세요: McMahan\n",
      "얼마나 많은 예측을 보고 싶나요? 10\n",
      "최상위 10개 예측:\n",
      "===================\n",
      "McMahan -> Irish (p=0.39)\n",
      "McMahan -> Scottish (p=0.26)\n",
      "McMahan -> Czech (p=0.07)\n",
      "McMahan -> Vietnamese (p=0.06)\n",
      "McMahan -> German (p=0.06)\n",
      "McMahan -> English (p=0.03)\n",
      "McMahan -> Dutch (p=0.03)\n",
      "McMahan -> Russian (p=0.03)\n",
      "McMahan -> French (p=0.02)\n",
      "McMahan -> Japanese (p=0.01)\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"분류하려는 성씨를 입력하세요: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "k = int(input(\"얼마나 많은 예측을 보고 싶나요? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"앗! 전체 국적 개수보다 큰 값을 입력했습니다. 모든 국적에 대한 예측을 반환합니다. :)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"최상위 {}개 예측:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526f32f",
   "metadata": {},
   "source": [
    "### 4.3 합성곱 신경망 (CNN)\n",
    "\n",
    "* 원핫 벡터의 크기는 입력 채널의 개수\n",
    "* 문자 시퀀스의 길이는 너비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a473e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 7])\n",
      "torch.Size([2, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "one_hot_size = 10\n",
    "sequence_width = 7\n",
    "data = torch.randn(batch_size, one_hot_size, sequence_width)\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=16, kernel_size=3)\n",
    "intermediate1 = conv1(data)\n",
    "print(data.size())\n",
    "print(intermediate1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ac17b",
   "metadata": {},
   "source": [
    "* 출력 텐서 크기 줄이는 방법  \n",
    "1) 합성곱 층을 더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feedc30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 3])\n",
      "torch.Size([2, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터에 반복 적용한 합성곱\n",
    "conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "\n",
    "intermediate2 = conv2(intermediate1)\n",
    "intermediate3 = conv3(intermediate2)\n",
    "\n",
    "print(intermediate2.size())\n",
    "print(intermediate3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ae28057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# 불필요한 차원 1 삭제\n",
    "y_output = intermediate3.squeeze()\n",
    "print(y_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7de000",
   "metadata": {},
   "source": [
    "2. 남은 값을 특성 벡터로 펼치기\n",
    "* 모든 정보 유지하지만, 필요 이상으로 큰 특성 벡터 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26a6cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80])\n"
     ]
    }
   ],
   "source": [
    "print(intermediate1.view(batch_size, -1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddc74d",
   "metadata": {},
   "source": [
    "3. 특성 맵 차원 따라 평균 계산하기\n",
    "* 특성 맵 차원의 크기에 상관없지만 일부 정보 잃을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8282de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(intermediate1, dim=2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c9194",
   "metadata": {},
   "source": [
    "### 4.4 예제 : CNN으로 성씨 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4b625534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary, SurnameDataset는 4.2와 같다.\n",
    "class SurnameVectorizer(object) :\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length) :\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length # 가장 긴 성씨 길이\n",
    "    \n",
    "    def vectorize(self, surname) :\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "        \n",
    "        for position_index, character in enumerate(surname) :\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df) :\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                   max_surname_length=contents['max_surname_length'])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable(), \n",
    "                'max_surname_length': self._max_surname_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f4da2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "class SurnameClassifier(nn.Module) :\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels) :\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(nn.Conv1d(in_channels=initial_num_channels,\n",
    "                                              out_channels = num_channels,\n",
    "                                              kernel_size=3),\n",
    "                                     \n",
    "                                    nn.ELU(),\n",
    "                                    nn.Conv1d(in_channels=num_channels,\n",
    "                                              out_channels=num_channels, \n",
    "                                             kernel_size=3, stride=2),\n",
    "                                     \n",
    "                                    nn.ELU(),\n",
    "                                    nn.Conv1d(in_channels=num_channels,\n",
    "                                              out_channels=num_channels, \n",
    "                                             kernel_size=3, stride=2),\n",
    "                                    \n",
    "                                    nn.ELU(),\n",
    "                                    nn.Conv1d(in_channels=num_channels,\n",
    "                                              out_channels=num_channels, \n",
    "                                             kernel_size=3, stride=2),\n",
    "                                     \n",
    "                                    nn.ELU()\n",
    "                                    )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x_surname, apply_softmax = False) :\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "        prediction_vector = self.fc(features)\n",
    "        \n",
    "        if apply_softmax :\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "            \n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26a3bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c681809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"./Data/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model1.pth\",\n",
    "    save_dir=\"./\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    catch_keyboard_interrupt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ee517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab), \n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               num_channels=args.num_channels)\n",
    "\n",
    "#classifer = classifier.to(args.device)\n",
    "#dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5eabca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "        loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                        epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # 검증 세트에 대한 순회\n",
    "\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       #device=args.device\n",
    "                                      )\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        y_pred =  classifier(batch_dict['x_surname'])\n",
    "        \n",
    "        loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                        epoch=epoch_index)\n",
    "        val_bar.update()\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=classifier,\n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d2021c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "#classifier = classifier.to(args.device)\n",
    "#dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   #device=args.device\n",
    "                                  )\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2579d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 1.95149365067482;\n",
      "테스트 정확도: 58.723958333333336\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4db69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "   \n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c59ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
